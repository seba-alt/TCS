---
phase: 02-rag-api
plan: 04
type: execute
wave: 3
depends_on:
  - "02-03"
files_modified:
  - app/routers/chat.py
  - app/main.py
autonomous: false
requirements:
  - REC-02

must_haves:
  truths:
    - "POST /api/chat streams SSE events — the browser receives the 'thinking' status immediately before generation completes"
    - "The first SSE event is always 'status: thinking' emitted within 100ms of the request arriving"
    - "The final SSE event contains the complete JSON payload (narrative + experts)"
    - "The SSE endpoint still logs the conversation to SQLite after generation completes"
    - "POST /api/chat with missing email still returns 422 (validation runs before streaming begins)"
  artifacts:
    - path: "app/routers/chat.py"
      provides: "POST /api/chat as SSE StreamingResponse with status:thinking event and final JSON payload"
      contains: "StreamingResponse"
    - path: "app/main.py"
      provides: "chat router updated for SSE; Content-Type text/event-stream"
      contains: "chat.router"
  key_links:
    - from: "app/routers/chat.py"
      to: "fastapi.responses.StreamingResponse"
      via: "StreamingResponse(generate_sse(...), media_type='text/event-stream')"
      pattern: "StreamingResponse"
    - from: "SSE generator"
      to: "app/services/llm.py generate_response()"
      via: "async generator yields thinking event then awaits generation"
      pattern: "yield.*thinking"
---

<objective>
Upgrade POST /api/chat from a blocking JSON response to a Server-Sent Events (SSE) stream. The endpoint emits a `status: thinking` event immediately, performs retrieval and generation, then emits the complete response as a final event. Verified end-to-end with a browser or curl --no-buffer.

Purpose: The CONTEXT.md requires `status: thinking` emitted immediately so the frontend can show a loading state the moment the request is received — not after the full Gemini response arrives (2-8 seconds later). SSE is the delivery mechanism; all RAG logic from Plan 03 is unchanged.
Output: POST /api/chat responds with Content-Type: text/event-stream; first event is status=thinking; final event is the complete recommendation JSON.
</objective>

<execution_context>
@/Users/sebastianhamers/.claude/get-shit-done/workflows/execute-plan.md
@/Users/sebastianhamers/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/02-rag-api/02-CONTEXT.md
@app/routers/chat.py
@app/main.py
@.planning/phases/02-rag-api/02-03-SUMMARY.md
</context>

<tasks>

<task type="auto">
  <name>Task 1: Convert POST /api/chat to SSE StreamingResponse</name>
  <files>app/routers/chat.py</files>
  <action>
    Replace the current blocking implementation in app/routers/chat.py with an SSE streaming version. The logic is identical — only the response delivery changes.

    Full replacement for app/routers/chat.py:

    ```python
    """
    POST /api/chat — SSE streaming expert recommendation endpoint.

    SSE event stream format:
        data: {"event": "status", "status": "thinking"}\n\n
        data: {"event": "result", "type": "...", "narrative": "...", "experts": [...]}\n\n
        data: {"event": "done"}\n\n

    On error during generation:
        data: {"event": "error", "message": "..."}\n\n
        data: {"event": "done"}\n\n

    Request body:
        email (str, required): User email — API enforces presence (lead capture).
        query (str, required): Natural language problem description.
        history (list[dict], optional): Prior turns [{role, content}] for multi-turn context.

    Error responses (before streaming starts):
        422: Missing or invalid email/query (Pydantic validation runs synchronously)
    """
    import asyncio
    import json

    import structlog
    from fastapi import APIRouter, Depends, Request
    from fastapi.responses import StreamingResponse
    from pydantic import BaseModel, EmailStr, Field
    from sqlalchemy.orm import Session

    from app.database import get_db
    from app.models import Conversation
    from app.services.llm import generate_response
    from app.services.retriever import retrieve

    log = structlog.get_logger()
    router = APIRouter()


    class HistoryItem(BaseModel):
        role: str
        content: str


    class ChatRequest(BaseModel):
        email: EmailStr
        query: str = Field(..., min_length=1, max_length=2000)
        history: list[HistoryItem] = Field(default_factory=list)


    def _sse(data: dict) -> str:
        """Format a dict as an SSE data line."""
        return f"data: {json.dumps(data)}\n\n"


    async def _stream_chat(body: ChatRequest, request: Request, db: Session):
        """
        Async generator yielding SSE events.

        Event sequence:
        1. status=thinking (immediate — before any LLM call)
        2. result (complete JSON payload after generation)
        3. done (signals stream end to client)
        """
        # Event 1: thinking — emit immediately so frontend shows loading state
        yield _sse({"event": "status", "status": "thinking"})

        # Run retrieval + generation in a thread pool to avoid blocking the event loop
        # (embed_query and generate_content are synchronous calls)
        loop = asyncio.get_event_loop()

        try:
            # Retrieve candidates (sync — run in thread pool)
            history_dicts = [{"role": h.role, "content": h.content} for h in body.history]
            candidates = await loop.run_in_executor(
                None,
                lambda: retrieve(
                    query=body.query,
                    faiss_index=request.app.state.faiss_index,
                    metadata=request.app.state.metadata,
                ),
            )
            log.info("chat.retrieved", candidate_count=len(candidates))

            # Generate response (sync — run in thread pool; has internal retry logic)
            llm_response = await loop.run_in_executor(
                None,
                lambda: generate_response(
                    query=body.query,
                    candidates=candidates,
                    history=history_dicts,
                ),
            )
            log.info("chat.generated", type=llm_response.type, expert_count=len(llm_response.experts))

            # Log conversation to DB
            conversation = Conversation(
                email=str(body.email),
                query=body.query,
                history=json.dumps(history_dicts),
                response_type=llm_response.type,
                response_narrative=llm_response.narrative,
                response_experts=json.dumps(
                    [
                        {
                            "name": e.name,
                            "title": e.title,
                            "company": e.company,
                            "hourly_rate": e.hourly_rate,
                            "profile_url": e.profile_url,
                        }
                        for e in llm_response.experts
                    ]
                ),
            )
            db.add(conversation)
            db.commit()
            log.info("chat.logged", conversation_id=conversation.id)

            # Event 2: result — complete recommendation payload
            experts_payload = [
                {
                    "name": e.name,
                    "title": e.title,
                    "company": e.company,
                    "hourly_rate": e.hourly_rate,
                    "profile_url": e.profile_url,
                }
                for e in llm_response.experts
            ]
            yield _sse({
                "event": "result",
                "type": llm_response.type,
                "narrative": llm_response.narrative,
                "experts": experts_payload,
            })

        except Exception as exc:
            log.error("chat.stream_error", error=str(exc))
            yield _sse({"event": "error", "message": "Failed to generate response. Please try again."})

        finally:
            # Event 3: done — always emitted to signal stream end
            yield _sse({"event": "done"})


    @router.post("/api/chat")
    async def chat(
        body: ChatRequest,
        request: Request,
        db: Session = Depends(get_db),
    ) -> StreamingResponse:
        """
        Stream expert recommendations as Server-Sent Events.

        Pydantic validation (email, query) runs synchronously before streaming begins —
        invalid requests return 422 before any SSE events are emitted.
        """
        return StreamingResponse(
            _stream_chat(body, request, db),
            media_type="text/event-stream",
            headers={
                "Cache-Control": "no-cache",
                "X-Accel-Buffering": "no",  # Disable nginx buffering (Railway uses nginx proxy)
            },
        )
    ```

    Note: `response_model` is removed from the route decorator because StreamingResponse bypasses Pydantic's response serialization. Validation is still enforced on the request body via ChatRequest.
  </action>
  <verify>
    Run: `python -c "from app.routers.chat import router, _sse, ChatRequest; print('SSE chat router OK:', len(router.routes), 'routes')"`
    Expected: "SSE chat router OK: 1 routes"

    Also verify the _sse formatter: `python -c "from app.routers.chat import _sse; import json; line = _sse({'event':'status','status':'thinking'}); assert line.startswith('data: ') and line.endswith('\n\n'); print('SSE format OK:', repr(line[:50]))"`
  </verify>
  <done>POST /api/chat returns StreamingResponse with media_type='text/event-stream'; _stream_chat generator yields thinking event first, then result, then done; DB logging preserved.</done>
</task>

<task type="checkpoint:human-verify" gate="blocking">
  <name>Task 2: Human verification — SSE streaming end-to-end</name>
  <what-built>
  <action>Start the server (uvicorn app.main:app --reload --port 8000) and run the curl verification tests listed in what-built below. Verify SSE event order, DB logging, error handling, and manual domain queries.</action>
    SSE streaming POST /api/chat endpoint. Start the server with:
    ```bash
    uvicorn app.main:app --reload --port 8000
    ```

    Then run the following curl test (--no-buffer shows events as they arrive):
    ```bash
    curl -s --no-buffer -X POST http://localhost:8000/api/chat \
      -H "Content-Type: application/json" \
      -d '{"email": "test@example.com", "query": "I need a machine learning engineer for my startup"}' \
      -N
    ```

    Expected SSE output (events arrive progressively — not all at once):
    ```
    data: {"event": "status", "status": "thinking"}

    data: {"event": "result", "type": "match", "narrative": "...", "experts": [...]}

    data: {"event": "done"}

    ```

    Run 3 additional validation tests:

    Test 2 — Missing email returns 422 (before streaming):
    ```bash
    curl -s -X POST http://localhost:8000/api/chat \
      -H "Content-Type: application/json" \
      -d '{"query": "I need an ML engineer"}'
    ```
    Expected: 422 JSON error (not an SSE stream)

    Test 3 — Vague query (may return clarification):
    ```bash
    curl -s --no-buffer -X POST http://localhost:8000/api/chat \
      -H "Content-Type: application/json" \
      -d '{"email": "test@example.com", "query": "help"}' \
      -N
    ```
    Expected: SSE events; if type=clarification, experts array is empty []

    Test 4 — DB logging check (after running Test 1):
    ```bash
    python3 -c "
    from app.database import SessionLocal
    from app.models import Conversation
    db = SessionLocal()
    count = db.query(Conversation).count()
    latest = db.query(Conversation).order_by(Conversation.id.desc()).first()
    print(f'Total conversations: {count}')
    print(f'Latest: {latest.email} | type={latest.response_type}')
    db.close()
    "
    ```
    Expected: at least 1 conversation in DB

    Test 5 — Phase 1 health still works:
    ```bash
    curl -s http://localhost:8000/api/health
    ```
    Expected: 200 with index_size > 0

    Test 6 — 10 manual queries (different domains):
    Try at least 3 different domain queries and verify plausible expert matches:
    - "I need a financial advisor for my Series A fundraise"
    - "Looking for a UX designer for our mobile app"
    - "Need help with cloud infrastructure on AWS"
    Each should return 3 plausible experts from the database with query-relevant explanations.
  </what-built>
  <how-to-verify>
    1. SSE stream structure: first event is status=thinking, last is done, middle is result
    2. "status: thinking" event arrives BEFORE the result (visible in curl --no-buffer output as separate lines)
    3. Result event contains: type ("match" or "clarification"), narrative (non-empty string), experts (array)
    4. Match responses: experts array has exactly 3 items, each with name/title/company/hourly_rate/profile_url fields
    5. Clarification responses: experts array is empty []
    6. Missing email returns 422 (not a streaming response)
    7. DB logging: at least 1 conversation row exists after running test queries
    8. Phase 1 health endpoint: still returns 200 with index_size > 0
    9. At least 3 different domain queries return plausible, non-hallucinated experts
  </how-to-verify>
  <resume-signal>Type "approved" if all tests pass. Describe any issues (wrong event order, missing experts, DB not logging, etc.) if problems found.</resume-signal>
</task>

</tasks>

<verification>
Post-approval verification — run after human signs off:
1. Confirm conversations.db has rows: `python3 -c "from app.database import SessionLocal; from app.models import Conversation; db = SessionLocal(); print(db.query(Conversation).count(), 'conversations logged'); db.close()"`
2. Confirm SSE header is set: `curl -s -I -X POST http://localhost:8000/api/chat -H "Content-Type: application/json" -d '{"email":"h@h.com","query":"test"}' | grep -i content-type`
   Expected: `content-type: text/event-stream`
3. Confirm 422 for missing email: `curl -s -o /dev/null -w "%{http_code}" -X POST http://localhost:8000/api/chat -H "Content-Type: application/json" -d '{}'`
   Expected: `422`
</verification>

<success_criteria>
- POST /api/chat streams SSE with Content-Type: text/event-stream
- First event in every response is {"event": "status", "status": "thinking"}
- Final event is always {"event": "done"}
- Middle event (result) contains type, narrative, experts matching CONTEXT.md schema
- For type=match: exactly 3 experts in array; for type=clarification: empty array
- Conversation logged to SQLite for every successful request
- Missing email still returns 422 (validation before streaming)
- Phase 1 /api/health unchanged
- Human verified: 10+ manual test queries return plausible, non-hallucinated expert matches
- Phase 2 success criteria from ROADMAP.md met:
  1. POST /api/chat returns valid JSON with exactly 3 experts (name, title, company, hourly rate, profile URL, why-them)
  2. Endpoint streams as SSE — status:thinking arrives before result
  3. 10 manual test queries cover different domains, each returning plausible experts
  4. Vague query triggers clarification response rather than forced match
</success_criteria>

<output>
After completion, create `.planning/phases/02-rag-api/02-04-SUMMARY.md`
</output>
