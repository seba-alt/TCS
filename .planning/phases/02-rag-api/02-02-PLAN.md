---
phase: 02-rag-api
plan: 02
type: execute
wave: 1
depends_on: []
files_modified:
  - app/services/retriever.py
  - app/services/llm.py
  - app/services/__init__.py
autonomous: true
requirements:
  - REC-02

must_haves:
  truths:
    - "Given a query string, the retriever returns up to 5 candidate experts with FAISS scores from app.state"
    - "The LLM service generates a response with exactly 3 experts when given a valid candidate list"
    - "The LLM service returns a clarification response when FAISS scores are below threshold"
    - "Gemini JSON mode returns structured data matching the expected response schema"
  artifacts:
    - path: "app/services/retriever.py"
      provides: "retrieve() function — embeds query, searches FAISS, filters incomplete experts, returns top candidates with scores"
      exports: ["retrieve", "RetrievedExpert"]
    - path: "app/services/llm.py"
      provides: "generate_response() function — calls gemini-2.5-flash with JSON mode, retries on failure, returns structured ChatResponse"
      exports: ["generate_response", "ChatResponse", "Expert"]
  key_links:
    - from: "app/services/retriever.py"
      to: "app/services/embedder.py"
      via: "embed_query() call"
      pattern: "embed_query\\("
    - from: "app/services/retriever.py"
      to: "app.state.faiss_index"
      via: "request.app.state passed in as argument"
      pattern: "faiss_index\\.search"
    - from: "app/services/llm.py"
      to: "google-genai SDK"
      via: "client.models.generate_content() with response_mime_type='application/json'"
      pattern: "generate_content"
---

<objective>
Build the two core services — retriever and LLM generator — that power the chat endpoint. These are pure service functions that can be tested and verified independently before the HTTP endpoint wires them together in Plan 03.

Purpose: Separating retrieval and generation into services makes them independently testable and keeps the route handler thin. The retriever handles FAISS search + incomplete data filtering. The LLM service handles Gemini JSON mode, retries, and response schema enforcement.
Output: app/services/retriever.py and app/services/llm.py — both callable standalone.
</objective>

<execution_context>
@/Users/sebastianhamers/.claude/get-shit-done/workflows/execute-plan.md
@/Users/sebastianhamers/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/02-rag-api/02-CONTEXT.md
@app/config.py
@app/services/embedder.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create retriever service</name>
  <files>app/services/retriever.py</files>
  <action>
    Create app/services/retriever.py:

    ```python
    """
    Retrieval service — embeds a query and searches the FAISS index for expert candidates.

    Key design decisions (from CONTEXT.md):
    - Returns up to TOP_K=5 candidates (the LLM picks the best 3 from these)
    - Experts missing name, title, company, or hourly_rate are omitted from results
    - Experts missing profile_url are included — profile_url is set to None
    - SIMILARITY_THRESHOLD: candidates below this score are flagged as low-confidence
    - The retriever does NOT make the final "clarification vs match" decision — it returns
      scores; the LLM service makes that call (per CONTEXT.md dual-check requirement)
    """
    from __future__ import annotations

    import json
    from dataclasses import dataclass, field
    from typing import TYPE_CHECKING

    import faiss
    import numpy as np

    from app.config import OUTPUT_DIM
    from app.services.embedder import embed_query

    if TYPE_CHECKING:
        pass

    # How many candidates to retrieve from FAISS before LLM selection.
    # Retrieve more than 3 to give the LLM room to skip low-quality matches.
    TOP_K = 5

    # Cosine similarity threshold (inner product on L2-normalized vectors = cosine similarity).
    # Candidates below this score trigger the dual low-confidence check path in the LLM service.
    # 0.65 is conservative — tune down if too many valid queries are flagged as low-confidence.
    SIMILARITY_THRESHOLD = 0.65


    @dataclass
    class RetrievedExpert:
        """A single expert candidate returned from FAISS search."""
        name: str
        title: str
        company: str
        hourly_rate: str
        profile_url: str | None
        score: float
        raw: dict = field(repr=False)  # Original metadata dict for LLM context


    def retrieve(query: str, faiss_index: faiss.IndexFlatIP, metadata: list[dict]) -> list[RetrievedExpert]:
        """
        Embed query, search FAISS, filter incomplete experts, return ranked candidates.

        Args:
            query: Natural language user query.
            faiss_index: Loaded FAISS index from app.state.faiss_index.
            metadata: Position-aligned metadata list from app.state.metadata.

        Returns:
            List of RetrievedExpert, sorted by score descending, length 0-TOP_K.
            An empty list means no valid candidates found — triggers clarification response.
        """
        # 1. Embed the query (L2-normalized 768-dim vector)
        vector = np.array(embed_query(query), dtype=np.float32).reshape(1, -1)

        # 2. Search FAISS — retrieve TOP_K nearest neighbors
        k = min(TOP_K, faiss_index.ntotal)
        scores, indices = faiss_index.search(vector, k)
        scores = scores[0].tolist()
        indices = indices[0].tolist()

        # 3. Build candidate list, skipping incomplete experts
        candidates: list[RetrievedExpert] = []
        for score, idx in zip(scores, indices):
            if idx < 0:  # FAISS returns -1 for unfilled slots when index has fewer than k vectors
                continue
            row = metadata[idx]

            # CONTEXT.md rule: omit experts missing name, title, company, or hourly_rate
            required = ["name", "title", "company", "hourly_rate"]
            # Normalize: look for common column name variants
            def _get(row: dict, *keys: str) -> str | None:
                for k in keys:
                    v = row.get(k) or row.get(k.replace("_", " "))
                    if v and str(v).strip() and str(v).strip().lower() not in ("nan", "none", ""):
                        return str(v).strip()
                return None

            name = _get(row, "name", "Name", "expert_name")
            title = _get(row, "title", "Title", "job_title", "position")
            company = _get(row, "company", "Company", "organization", "employer")
            hourly_rate = _get(row, "hourly_rate", "hourly rate", "rate", "Rate", "price")

            if not all([name, title, company, hourly_rate]):
                continue  # Skip experts missing required fields

            # CONTEXT.md rule: include expert even if profile_url is missing; just omit the link
            profile_url = _get(row, "profile_url", "profile url", "url", "URL", "link")

            candidates.append(RetrievedExpert(
                name=name,
                title=title,
                company=company,
                hourly_rate=hourly_rate,
                profile_url=profile_url,
                score=score,
                raw=row,
            ))

        return candidates
    ```

    Note: The column name normalization uses common variants because CONTEXT.md states column names were treated as unknown until validate_csv.py ran. The `_get()` helper handles snake_case, space-separated, and Title Case variants. After Phase 1's validate_csv.py run, the executor should check the actual column names in metadata.json and update the `_get()` calls if needed — the current implementation is defensive.
  </action>
  <verify>
    Run: `python -c "from app.services.retriever import retrieve, RetrievedExpert, SIMILARITY_THRESHOLD, TOP_K; print('Retriever OK, threshold:', SIMILARITY_THRESHOLD)"`
    Expected: prints "Retriever OK, threshold: 0.65"

    Note: A live FAISS search test requires the server to be running (faiss_index comes from app.state). The full integration test happens in Plan 03 when the endpoint is wired. Import success is the correct verification here.
  </verify>
  <done>app/services/retriever.py importable; retrieve(), RetrievedExpert, SIMILARITY_THRESHOLD, TOP_K all exported; no import errors.</done>
</task>

<task type="auto">
  <name>Task 2: Create LLM generation service with Gemini JSON mode</name>
  <files>app/services/llm.py</files>
  <action>
    Create app/services/llm.py:

    ```python
    """
    LLM generation service — calls Gemini to produce structured expert recommendations.

    Key design decisions (from CONTEXT.md):
    - Model: gemini-2.5-flash (NOT gemini-2.0-flash — deprecated, shutdown June 2026)
    - Response format: JSON mode via response_mime_type="application/json"
    - Response schema: {"type": "match"|"clarification", "narrative": str, "experts": [...]}
    - "Why them" style: 1-2 sentences, query-specific, grounded in title/company/domain
    - Low-confidence path: BOTH score threshold AND LLM judgment — score check runs first (fast)
    - Retry: up to 3 attempts with exponential backoff on Gemini API failure
    - History: last N messages as context (sliding window, configurable via HISTORY_WINDOW)
    """
    from __future__ import annotations

    import json
    import time
    from dataclasses import dataclass, field

    import structlog
    from google import genai
    from google.genai import types

    from app.services.retriever import RetrievedExpert, SIMILARITY_THRESHOLD

    log = structlog.get_logger()

    GENERATION_MODEL = "gemini-2.5-flash"  # Do NOT use gemini-2.0-flash (deprecated)
    MAX_RETRIES = 3
    RETRY_BASE_DELAY = 1.0  # seconds; doubles each retry (1s, 2s, 4s)

    # Number of prior conversation turns to include as context.
    # Each "turn" = one user query + one assistant response.
    # Configurable: set HISTORY_WINDOW=0 to disable multi-turn context.
    HISTORY_WINDOW = 3


    @dataclass
    class Expert:
        name: str
        title: str
        company: str
        hourly_rate: str
        profile_url: str | None


    @dataclass
    class ChatResponse:
        type: str  # "match" | "clarification"
        narrative: str
        experts: list[Expert] = field(default_factory=list)


    # Lazy Gemini client (same pattern as embedder.py — deferred so module imports without API key)
    _client: genai.Client | None = None


    def _get_client() -> genai.Client:
        global _client
        if _client is None:
            _client = genai.Client()
        return _client


    def _build_prompt(
        query: str,
        candidates: list[RetrievedExpert],
        history: list[dict],
        is_low_confidence: bool,
    ) -> str:
        """Build the Gemini prompt from query, candidates, and conversation history."""
        history_text = ""
        if history:
            turns = history[-HISTORY_WINDOW * 2:]  # Each turn = 2 entries (user + assistant)
            history_text = "\n".join(
                f"{'User' if m['role'] == 'user' else 'Assistant'}: {m['content']}"
                for m in turns
            )
            history_text = f"\n\nConversation history (most recent {HISTORY_WINDOW} turns):\n{history_text}"

        candidates_text = "\n".join(
            f"- {c.name} | {c.title} at {c.company} | Rate: {c.hourly_rate} | "
            f"URL: {c.profile_url or 'N/A'} | Similarity: {c.score:.3f}"
            for c in candidates
        )

        if is_low_confidence or not candidates:
            return f"""You are a professional concierge matching users with expert consultants.

    The user's query did not produce high-confidence expert matches from the database.
    Query: "{query}"{history_text}

    Retrieved candidates (low confidence — scores below {SIMILARITY_THRESHOLD}):
    {candidates_text if candidates else "(none)"}

    First, evaluate whether these candidates actually address the user's problem. If the match quality is poor, respond with type="clarification" and ask one targeted follow-up question to better understand their need. If candidates are usable despite low scores, respond with type="match" and recommend the best 3.

    Return a JSON object with exactly these fields:
    - "type": "clarification" or "match"
    - "narrative": your response text (for clarification: a single follow-up question; for match: 1-2 sentence intro then 3 expert recommendations)
    - "experts": for "match" — array of exactly 3 objects; for "clarification" — empty array []

    Each expert object: {{"name": str, "title": str, "company": str, "hourly_rate": str, "profile_url": str or null}}

    For match responses: each "Why them" explanation must be 1-2 sentences, reference the expert's specific title/company/domain, and tie directly to what the user asked. No hallucinated details — only use the data provided above."""

        return f"""You are a professional concierge matching users with expert consultants.

    User query: "{query}"{history_text}

    Top candidate experts from our database:
    {candidates_text}

    Select the 3 best-matched experts from this list. Write a response with:
    1. A brief 1-2 sentence intro acknowledging the user's need
    2. For each of the 3 experts: a "Why them:" explanation of 1-2 sentences that references their specific title, company, or domain and explains why they fit this exact query. No hallucinated details — only use the data provided above.

    Return a JSON object with exactly these fields:
    - "type": "match"
    - "narrative": the full combined text (intro + all 3 expert explanations as one prose block)
    - "experts": array of exactly 3 objects: {{"name": str, "title": str, "company": str, "hourly_rate": str, "profile_url": str or null}}"""


    def generate_response(
        query: str,
        candidates: list[RetrievedExpert],
        history: list[dict],
    ) -> ChatResponse:
        """
        Generate expert recommendations or a clarifying question via Gemini.

        Args:
            query: The user's natural language query.
            candidates: Retrieved expert candidates from the retriever service.
            history: Prior conversation turns [{role: "user"|"assistant", content: str}].
                     Sliding window — pass the full history; this function trims to HISTORY_WINDOW.

        Returns:
            ChatResponse with type, narrative, and experts list.

        Raises:
            RuntimeError: If all retries are exhausted and Gemini still fails.
        """
        # Score-based low-confidence check (fast path — runs before calling LLM)
        is_low_confidence = (
            not candidates
            or all(c.score < SIMILARITY_THRESHOLD for c in candidates)
        )

        prompt = _build_prompt(query, candidates, history, is_low_confidence)

        last_error: Exception | None = None
        for attempt in range(MAX_RETRIES):
            try:
                response = _get_client().models.generate_content(
                    model=GENERATION_MODEL,
                    contents=prompt,
                    config=types.GenerateContentConfig(
                        response_mime_type="application/json",
                        temperature=0.3,  # Low temperature for consistent structured output
                    ),
                )
                raw_json = response.text
                data = json.loads(raw_json)

                response_type = data.get("type", "match")
                narrative = data.get("narrative", "")
                experts_raw = data.get("experts", [])

                experts = [
                    Expert(
                        name=e.get("name", ""),
                        title=e.get("title", ""),
                        company=e.get("company", ""),
                        hourly_rate=e.get("hourly_rate", ""),
                        profile_url=e.get("profile_url") or None,
                    )
                    for e in experts_raw
                ]

                log.info(
                    "llm.generate_response",
                    type=response_type,
                    expert_count=len(experts),
                    attempt=attempt + 1,
                )
                return ChatResponse(type=response_type, narrative=narrative, experts=experts)

            except Exception as exc:
                last_error = exc
                delay = RETRY_BASE_DELAY * (2 ** attempt)
                log.warning(
                    "llm.generate_response.retry",
                    attempt=attempt + 1,
                    max_retries=MAX_RETRIES,
                    delay=delay,
                    error=str(exc),
                )
                if attempt < MAX_RETRIES - 1:
                    time.sleep(delay)

        raise RuntimeError(
            f"Gemini generation failed after {MAX_RETRIES} attempts: {last_error}"
        )
    ```
  </action>
  <verify>
    Run: `python -c "
    from app.services.llm import generate_response, ChatResponse, Expert, GENERATION_MODEL, HISTORY_WINDOW
    print('LLM service OK')
    print('Model:', GENERATION_MODEL)
    print('History window:', HISTORY_WINDOW)
    # Verify dataclasses are constructable
    e = Expert(name='Test', title='Engineer', company='Acme', hourly_rate='$100/hr', profile_url=None)
    r = ChatResponse(type='match', narrative='Test narrative', experts=[e])
    print('ChatResponse:', r.type, len(r.experts), 'experts')
    "`
    Expected output:
    ```
    LLM service OK
    Model: gemini-2.5-flash
    History window: 3
    ChatResponse: match 1 experts
    ```
  </verify>
  <done>app/services/llm.py importable; generate_response, ChatResponse, Expert exported; GENERATION_MODEL is "gemini-2.5-flash"; no import errors without GOOGLE_API_KEY (lazy client).</done>
</task>

</tasks>

<verification>
Run the full verification sequence:
1. `python -c "from app.services.retriever import retrieve, RetrievedExpert, SIMILARITY_THRESHOLD; print('Retriever OK')"` — no errors
2. `python -c "from app.services.llm import generate_response, ChatResponse, Expert; print('LLM OK')"` — no errors
3. Confirm GENERATION_MODEL is "gemini-2.5-flash" (not gemini-2.0-flash): `python -c "from app.services.llm import GENERATION_MODEL; assert GENERATION_MODEL == 'gemini-2.5-flash', f'Wrong model: {GENERATION_MODEL}'; print('Model OK:', GENERATION_MODEL)"`
4. Confirm lazy client: `python -c "import os; os.environ.pop('GOOGLE_API_KEY', None); from app.services.llm import generate_response; print('No-key import OK')"` — no error (client only created on first call)
</verification>

<success_criteria>
- app/services/retriever.py: retrieve() accepts (query, faiss_index, metadata), returns list[RetrievedExpert]; TOP_K=5; SIMILARITY_THRESHOLD=0.65
- app/services/llm.py: generate_response() accepts (query, candidates, history), returns ChatResponse; GENERATION_MODEL="gemini-2.5-flash"; MAX_RETRIES=3; lazy client init
- Both modules importable without GOOGLE_API_KEY set (lazy client pattern from embedder.py)
- LLM prompt enforces: 1-2 sentence "why them" explanations; query-specific + profile-grounded; narrative as single combined prose block; response JSON schema {type, narrative, experts}
</success_criteria>

<output>
After completion, create `.planning/phases/02-rag-api/02-02-SUMMARY.md`
</output>
