---
phase: 02-rag-api
plan: 01
type: execute
wave: 1
depends_on: []
files_modified:
  - requirements.txt
  - app/config.py
  - app/database.py
  - app/models.py
  - app/main.py
autonomous: true
requirements:
  - REC-02

must_haves:
  truths:
    - "A Conversation record can be created with email, query, response, and type fields"
    - "The database table exists and is created automatically at server startup"
    - "The SQLAlchemy session is accessible to route handlers via FastAPI dependency injection"
  artifacts:
    - path: "app/database.py"
      provides: "SQLAlchemy engine, SessionLocal factory, and get_db dependency for FastAPI"
      exports: ["engine", "SessionLocal", "get_db", "Base"]
    - path: "app/models.py"
      provides: "Conversation SQLAlchemy ORM model with all required fields"
      contains: "class Conversation"
    - path: "app/config.py"
      provides: "DATABASE_URL constant pointing to data/conversations.db"
      contains: "DATABASE_URL"
  key_links:
    - from: "app/main.py"
      to: "app/database.py"
      via: "Base.metadata.create_all(bind=engine) in lifespan startup"
      pattern: "Base\\.metadata\\.create_all"
    - from: "app/routers/chat.py (future)"
      to: "app/database.py"
      via: "Depends(get_db) in route handler signature"
      pattern: "Depends\\(get_db\\)"
---

<objective>
Set up SQLite database with SQLAlchemy ORM for conversation logging. The email capture and conversation persistence requirement (every conversation must be stored) needs a database layer before any route logic is written.

Purpose: The CONTEXT.md requires ALL conversations (email + query + response) to be persisted — this is a lead capture and analytics requirement. The DB layer must exist before the chat endpoint can be implemented in Plan 03.
Output: SQLite database via SQLAlchemy, Conversation model, FastAPI get_db dependency, table auto-creation at startup.
</objective>

<execution_context>
@/Users/sebastianhamers/.claude/get-shit-done/workflows/execute-plan.md
@/Users/sebastianhamers/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/02-rag-api/02-CONTEXT.md
@app/config.py
@app/main.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Add SQLAlchemy to requirements and create database module</name>
  <files>requirements.txt, app/database.py, app/config.py</files>
  <action>
    1. Add `sqlalchemy==2.0.*` to requirements.txt (after the existing dependencies, before dev section). Do NOT add alembic — migrations are overkill for v1; use Base.metadata.create_all() at startup.

    2. Run `pip install "sqlalchemy==2.0.*"` to install into the local environment.

    3. Add to app/config.py (after the existing METADATA_PATH constant):
    ```python
    # SQLite database for conversation logging.
    # NOTE: SQLite writes to the container's ephemeral filesystem on Railway.
    # For production persistence, replace with DATABASE_URL env var pointing to Postgres.
    # For v1, SQLite is sufficient for lead capture and analytics.
    DATABASE_URL = f"sqlite:///{DATA_DIR / 'conversations.db'}"
    ```

    4. Create app/database.py:
    ```python
    """
    SQLAlchemy database setup.

    Uses SQLite for v1 (file-based, zero-config).
    Railway note: SQLite writes to ephemeral container storage — data survives restarts
    but not redeployments. Replace DATABASE_URL with a managed Postgres URL for
    production durability (Phase 4 concern).

    get_db() is the FastAPI dependency that provides a DB session per request.
    """
    from sqlalchemy import create_engine
    from sqlalchemy.orm import DeclarativeBase, sessionmaker

    from app.config import DATABASE_URL

    engine = create_engine(
        DATABASE_URL,
        connect_args={"check_same_thread": False},  # Required for SQLite with FastAPI threads
    )

    SessionLocal = sessionmaker(autocommit=False, autoflush=False, bind=engine)


    class Base(DeclarativeBase):
        pass


    def get_db():
        """FastAPI dependency — yields a DB session and closes it after the request."""
        db = SessionLocal()
        try:
            yield db
        finally:
            db.close()
    ```
  </action>
  <verify>
    Run: `python -c "from app.database import engine, SessionLocal, get_db, Base; print('DB module OK')"`
    Expected: prints "DB module OK" with no errors.
  </verify>
  <done>app/database.py importable, DATABASE_URL in config.py, sqlalchemy installed, no import errors.</done>
</task>

<task type="auto">
  <name>Task 2: Create Conversation ORM model and wire table creation into lifespan</name>
  <files>app/models.py, app/main.py</files>
  <action>
    1. Create app/models.py:
    ```python
    """
    SQLAlchemy ORM models.

    Conversation: persists every user interaction for lead capture and analytics.
    Email is required — the chat endpoint enforces this at request validation time.
    """
    import datetime

    from sqlalchemy import DateTime, String, Text
    from sqlalchemy.orm import Mapped, mapped_column

    from app.database import Base


    class Conversation(Base):
        __tablename__ = "conversations"

        id: Mapped[int] = mapped_column(primary_key=True, index=True)
        email: Mapped[str] = mapped_column(String(320), index=True, nullable=False)
        query: Mapped[str] = mapped_column(Text, nullable=False)
        # history: JSON-serialized list of prior {role, content} dicts for multi-turn context.
        # Empty list ("[]") for first-turn queries.
        history: Mapped[str] = mapped_column(Text, nullable=False, default="[]")
        response_type: Mapped[str] = mapped_column(String(50), nullable=False)  # "match" | "clarification"
        response_narrative: Mapped[str] = mapped_column(Text, nullable=True)
        # experts: JSON-serialized list of expert dicts (name, title, company, hourly_rate, profile_url).
        # Empty list ("[]") for clarification responses.
        response_experts: Mapped[str] = mapped_column(Text, nullable=False, default="[]")
        created_at: Mapped[datetime.datetime] = mapped_column(
            DateTime, default=datetime.datetime.utcnow, nullable=False
        )
    ```

    2. Update app/main.py lifespan to create DB tables at startup. Add these imports at the top of app/main.py:
    ```python
    from app.database import Base, engine
    import app.models  # noqa: F401 — registers ORM models with Base metadata
    ```
    Then inside the lifespan function, BEFORE the FAISS loading block, add:
    ```python
    # Create database tables if they don't exist (idempotent — safe to call on every startup)
    Base.metadata.create_all(bind=engine)
    log.info("startup: database tables created/verified")
    ```
    Keep all existing FAISS loading code intact. Do not change CORS, router registration, or any other existing code.
  </action>
  <verify>
    1. Run: `python -c "from app.models import Conversation; print('Model OK:', Conversation.__tablename__)"`
       Expected output: `Model OK: conversations`

    2. Run: `python -c "
    from app.database import engine, Base
    import app.models
    Base.metadata.create_all(bind=engine)
    from sqlalchemy import inspect
    inspector = inspect(engine)
    cols = [c['name'] for c in inspector.get_columns('conversations')]
    print('Columns:', cols)
    "`
    Expected: prints all column names including id, email, query, history, response_type, response_narrative, response_experts, created_at.

    3. Confirm data/conversations.db file is created after the above command runs.
  </verify>
  <done>Conversation model defined with all required fields; Base.metadata.create_all() runs in lifespan; conversations.db created on startup.</done>
</task>

</tasks>

<verification>
Run the full verification sequence:
1. `python -c "from app.database import engine, SessionLocal, get_db, Base; print('DB OK')"` — no errors
2. `python -c "from app.models import Conversation; print(Conversation.__tablename__)"` — prints "conversations"
3. `python -c "import app.models; from app.database import Base, engine; Base.metadata.create_all(bind=engine); print('Tables created')"` — no errors, data/conversations.db created
4. Check that app/main.py still imports and starts without errors: `python -c "from app.main import app; print('App import OK')"`
5. Confirm data/conversations.db is listed in .gitignore (it should be covered by the data/ wildcard rules already; if not, add data/conversations.db to .gitignore)
</verification>

<success_criteria>
- sqlalchemy==2.0.* installed and in requirements.txt
- app/database.py provides engine, SessionLocal, get_db, Base with no import errors
- app/config.py has DATABASE_URL pointing to data/conversations.db
- app/models.py defines Conversation with: id, email, query, history, response_type, response_narrative, response_experts, created_at
- app/main.py lifespan calls Base.metadata.create_all(bind=engine) at startup
- data/conversations.db created when the above verification commands run
- Existing Phase 1 functionality (FAISS loading, /api/health) unchanged
</success_criteria>

<output>
After completion, create `.planning/phases/02-rag-api/02-01-SUMMARY.md`
</output>
