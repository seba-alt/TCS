---
phase: 08-test-lab-run-queries-against-search-engine-evaluate-results-iterate-on-retrieval-quality
plan: "03"
type: execute
wave: 2
depends_on:
  - "08-01"
files_modified:
  - scripts/ingest.py
autonomous: true
requirements:
  - TAGS-03
  - TAGS-04

must_haves:
  truths:
    - "Running scripts/ingest.py reads experts from the SQLite Expert table (not experts.csv), filters to only tagged experts (tags IS NOT NULL), and builds a FAISS index"
    - "The embedding text for each expert includes their tags as 'Domains: tag1, tag2, tag3.' appended after bio"
    - "The FAISS index is written to a staging path first, then an assertion checks index.ntotal == actual_tagged_count before promoting (renaming) to the production path — crash-safe promotion"
    - "Any existing *.staging file is deleted at script start to prevent stale staging files from a previous crashed run"
    - "metadata.json is updated to include tags for each indexed expert (preserves existing retriever.py lookup pattern)"
  artifacts:
    - path: "scripts/ingest.py"
      provides: "DB-sourced FAISS ingest with tag enrichment and crash-safe index promotion"
      min_lines: 80
  key_links:
    - from: "scripts/ingest.py"
      to: "app/database.py"
      via: "imports SessionLocal to read Expert rows"
      pattern: "from app.database import SessionLocal"
    - from: "scripts/ingest.py"
      to: "app/models.py"
      via: "imports Expert model for typed queries"
      pattern: "from app.models import Expert"
    - from: "scripts/ingest.py"
      to: "data/faiss.index"
      via: "writes index via staging then rename"
      pattern: "STAGING_PATH\\.rename"
---

<objective>
Rewrite scripts/ingest.py to read experts from the SQLite Expert table instead of experts.csv, add domain tags to each expert's embedding text, and implement crash-safe FAISS index promotion via a staging file with count assertion.

Purpose: After Phase 8 tagging, the FAISS index must be rebuilt from DB-sourced data with tag-enriched embeddings. The staging + assertion pattern ensures a partially-written index never replaces the production index.

Output: Rewritten scripts/ingest.py. The old CSV-based logic is replaced; the embedding + FAISS build logic is preserved.
</objective>

<execution_context>
@/Users/sebastianhamers/.claude/get-shit-done/workflows/execute-plan.md
@/Users/sebastianhamers/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/08-test-lab-run-queries-against-search-engine-evaluate-results-iterate-on-retrieval-quality/08-CONTEXT.md
@.planning/phases/08-test-lab-run-queries-against-search-engine-evaluate-results-iterate-on-retrieval-quality/08-RESEARCH.md
@.planning/phases/08-test-lab-run-queries-against-search-engine-evaluate-results-iterate-on-retrieval-quality/08-01-SUMMARY.md
</context>

<tasks>

<task type="auto">
  <name>Task 1: Rewrite scripts/ingest.py — DB-sourced with tag enrichment and crash-safe promotion</name>
  <files>scripts/ingest.py</files>
  <action>
Rewrite scripts/ingest.py completely. Preserve all embedding and FAISS logic (embed_batch, IndexFlatIP, L2 normalize, tenacity retry, EMBEDDING_MODEL, OUTPUT_DIM, INGEST_BATCH_SIZE from app.config). Replace the CSV loading and direct-write logic.

Key changes from the current file:
1. Remove pandas import and CSV reading logic entirely
2. Add SQLAlchemy imports for SessionLocal + Expert model
3. Replace expert_to_text to accept a dict (same as before) but add tags field
4. Add load_tagged_experts() to query DB
5. Replace main() to use DB source + staging write + assertion + rename

Full rewrite:

```python
#!/usr/bin/env python3
"""
Offline ingestion: Expert DB table -> FAISS index + metadata JSON.

Run AFTER scripts/tag_experts.py has tagged experts:
  python scripts/ingest.py

NEVER call this at API startup — it takes 60+ seconds and hits the embedding API.

Source: SQLAlchemy Expert table (NOT experts.csv — tags written by tag_experts.py
are included in the embedding text only when reading from DB).

Only tagged experts (tags IS NOT NULL) are indexed. Experts with no bio are
excluded from tagging and therefore excluded from the FAISS index.

Index promotion: written to staging path first, count assertion checked,
then atomically renamed to production path. Prevents a partial write from
corrupting the production index.
"""
import json
import sys
import time
from pathlib import Path

import faiss
import numpy as np
from dotenv import load_dotenv
from google import genai
from google.genai import types
from tenacity import retry, stop_after_attempt, wait_exponential

# Load .env for local development — no-op in production
load_dotenv()

# Import app modules — must run from repo root
sys.path.insert(0, str(Path(__file__).resolve().parent.parent))

from app.config import (  # noqa: E402
    EMBEDDING_MODEL,
    FAISS_INDEX_PATH,
    INGEST_BATCH_SIZE,
    METADATA_PATH,
    OUTPUT_DIM,
)
from app.database import SessionLocal  # noqa: E402
from app.models import Expert  # noqa: E402
from sqlalchemy import select  # noqa: E402

client = genai.Client()

STAGING_PATH = FAISS_INDEX_PATH.with_suffix(".staging")


def load_tagged_experts() -> list[dict]:
    """
    Load all Expert rows where tags IS NOT NULL.
    Returns list of dicts for use in expert_to_text() and metadata.
    """
    with SessionLocal() as db:
        experts = db.scalars(
            select(Expert).where(Expert.tags.isnot(None))
        ).all()
        return [
            {
                "id": e.id,
                "username": e.username,
                "First Name": e.first_name,
                "Last Name": e.last_name,
                "Job Title": e.job_title,
                "Company": e.company,
                "Bio": e.bio,
                "Hourly Rate": e.hourly_rate,
                "Currency": e.currency,
                "Profile URL": e.profile_url,
                "Profile URL with UTM": e.profile_url_utm,
                "tags": json.loads(e.tags or "[]"),
                "findability_score": e.findability_score,
                "category": e.category,
            }
            for e in experts
        ]


def expert_to_text(expert: dict) -> str:
    """
    Construct semantically rich embedding text from an expert dict.
    Tags are appended as 'Domains: tag1, tag2, tag3.' for richer semantic signal.
    """
    first = str(expert.get("First Name") or "").strip()
    last = str(expert.get("Last Name") or "").strip()
    name = f"{first} {last}".strip()
    title = str(expert.get("Job Title") or "").strip()
    company = str(expert.get("Company") or "").strip()
    bio = str(expert.get("Bio") or "").strip()
    tags: list[str] = expert.get("tags") or []

    parts = []
    if name:
        parts.append(f"{name}.")
    if title and company:
        parts.append(f"{title} at {company}.")
    elif title:
        parts.append(f"{title}.")
    elif company:
        parts.append(f"Works at {company}.")
    if bio:
        parts.append(bio)
    if tags:
        parts.append(f"Domains: {', '.join(tags)}.")

    return " ".join(parts) if parts else name or expert.get("username", "Unknown expert")


@retry(
    stop=stop_after_attempt(5),
    wait=wait_exponential(multiplier=1, min=4, max=60),
    reraise=True,
)
def embed_batch(texts: list[str]) -> list[list[float]]:
    """
    Embed a batch of texts with tenacity retry for 429 rate limit errors.
    Returns list of OUTPUT_DIM-length float vectors.
    """
    result = client.models.embed_content(
        model=EMBEDDING_MODEL,
        contents=texts,
        config=types.EmbedContentConfig(
            task_type="RETRIEVAL_DOCUMENT",
            output_dimensionality=OUTPUT_DIM,
        ),
    )
    return [e.values for e in result.embeddings]


def build_index(experts: list[dict]) -> tuple[faiss.IndexFlatIP, list[dict]]:
    """
    Embed all experts in batches and build a FAISS IndexFlatIP.
    Applies L2 normalization (required for truncated-dim cosine similarity).
    """
    all_vectors: list[list[float]] = []
    total = len(experts)

    for i in range(0, total, INGEST_BATCH_SIZE):
        batch = experts[i:i + INGEST_BATCH_SIZE]
        texts = [expert_to_text(e) for e in batch]

        try:
            vectors = embed_batch(texts)
        except Exception as e:
            print(f"[error] Batch {i}-{i + len(batch)} failed after retries: {e}")
            raise

        all_vectors.extend(vectors)

        done = min(i + INGEST_BATCH_SIZE, total)
        print(f"  Embedded {done}/{total} experts ({done * 100 // total}%)")

        if i + INGEST_BATCH_SIZE < total:
            time.sleep(0.5)

    matrix = np.array(all_vectors, dtype=np.float32)
    faiss.normalize_L2(matrix)

    index = faiss.IndexFlatIP(OUTPUT_DIM)
    index.add(matrix)

    return index, experts  # Return original experts dicts as metadata


def main() -> None:
    # Clean up any stale staging file from a previous crashed run
    if STAGING_PATH.exists():
        STAGING_PATH.unlink()
        print(f"Removed stale staging file: {STAGING_PATH}")

    print("Loading tagged experts from DB...")
    experts = load_tagged_experts()
    actual_count = len(experts)

    if actual_count == 0:
        print("[error] No tagged experts found in DB. Run scripts/tag_experts.py first.")
        sys.exit(1)

    print(f"  {actual_count} tagged experts loaded (of 1558 total; {1558 - actual_count} skipped — no bio or untagged)")
    print()
    print(f"Embedding {actual_count} experts in batches of {INGEST_BATCH_SIZE}...")
    print(f"  Model: {EMBEDDING_MODEL}, dim: {OUTPUT_DIM}")
    print()

    index, metadata = build_index(experts)

    # Crash-safe promotion: write to staging, assert count, then rename to production path
    FAISS_INDEX_PATH.parent.mkdir(parents=True, exist_ok=True)

    print()
    print("Writing index to staging path...")
    faiss.write_index(index, str(STAGING_PATH))
    print(f"  Staging: {STAGING_PATH} ({index.ntotal} vectors)")

    # Assert before promoting — never overwrite production with a mismatched index
    assert index.ntotal == actual_count, (
        f"Index count mismatch: {index.ntotal} != {actual_count}. "
        f"Staging file kept at {STAGING_PATH} for inspection."
    )

    STAGING_PATH.rename(FAISS_INDEX_PATH)
    print(f"  Promoted to: {FAISS_INDEX_PATH}")

    # Write metadata.json — includes tags for each expert (preserves retriever.py lookup pattern)
    with open(METADATA_PATH, "w", encoding="utf-8") as f:
        json.dump(metadata, f, ensure_ascii=False, indent=None, default=str)
    print(f"  Metadata: {METADATA_PATH} ({len(metadata)} records)")

    print()
    print(f"Ingestion complete: {index.ntotal} experts indexed at {OUTPUT_DIM} dims.")


if __name__ == "__main__":
    main()
```

Implementation notes:
- pandas import is removed entirely — replaced by SQLAlchemy query
- The metadata dicts use "First Name", "Last Name" etc. (capital + spaced) to maintain compatibility with existing retriever.py / llm.py which look up metadata by those keys. This matches the metadata.json field names documented in MEMORY.md.
- STAGING_PATH = FAISS_INDEX_PATH.with_suffix(".staging") — uses the same directory as the production index
- The assertion uses actual_count from the DB query, not hardcoded 1558 (per user decision in CONTEXT.md and Pitfall 7 in RESEARCH.md)
- The stale staging file cleanup at start prevents confusion if a previous run crashed after writing the staging file
  </action>
  <verify>
Run: python -c "import ast; ast.parse(open('scripts/ingest.py').read()); print('syntax ok')"
Expected: syntax ok

Run: python -c "
import sys; sys.path.insert(0, '.')
from scripts.ingest import load_tagged_experts, expert_to_text
# Test expert_to_text with tags
sample = {'First Name': 'Jane', 'Last Name': 'Doe', 'Job Title': 'ML Engineer', 'Company': 'ACME', 'Bio': 'Expert in NLP', 'tags': ['machine learning', 'nlp']}
text = expert_to_text(sample)
print('tags in text:', 'Domains:' in text and 'machine learning' in text)
"
Expected: tags in text: True

Run: python -c "
import sys; sys.path.insert(0, '.')
import scripts.ingest as m
print('pandas import removed:', 'pandas' not in dir(m))
print('SessionLocal imported:', hasattr(m, 'SessionLocal'))
"
Expected: pandas import removed: True, SessionLocal imported: True
  </verify>
  <done>scripts/ingest.py has valid syntax, loads experts from DB (not CSV), includes tags in embedding text, writes to staging path then asserts count before rename, and removes stale staging files on startup.</done>
</task>

</tasks>

<verification>
1. python -c "import ast; ast.parse(open('scripts/ingest.py').read())" → no SyntaxError
2. grep "pd.read_csv\|import pandas" scripts/ingest.py → no matches (CSV logic removed)
3. grep "SessionLocal\|load_tagged_experts" scripts/ingest.py → both present
4. grep "STAGING_PATH\|staging" scripts/ingest.py → staging write + rename + stale cleanup all present
5. grep "assert index.ntotal == actual_count" scripts/ingest.py → assertion present
6. grep "Domains:" scripts/ingest.py → tag enrichment present in expert_to_text
</verification>

<success_criteria>
- scripts/ingest.py reads from Expert DB table, not experts.csv
- expert_to_text includes "Domains: tag1, tag2." for experts with tags
- Index written to staging path first, assertion index.ntotal == actual_count before promotion
- Stale staging file deleted at script start
- metadata.json written with tags included per expert record
- pandas dependency removed from ingest.py (though still in requirements.txt for other potential use)
</success_criteria>

<output>
After completion, create `.planning/phases/08-test-lab-run-queries-against-search-engine-evaluate-results-iterate-on-retrieval-quality/08-03-SUMMARY.md`
</output>
