---
phase: 10-search-intelligence-layer
plan: "01"
type: execute
wave: 1
depends_on: []
files_modified:
  - app/services/search_intelligence.py
autonomous: true
requirements:
  - SEARCH-01
  - SEARCH-02
  - SEARCH-03
  - SEARCH-04
  - SEARCH-05
  - SEARCH-06

must_haves:
  truths:
    - "When QUERY_EXPANSION_ENABLED=false, retrieve_with_intelligence() returns original candidates unchanged with hyde_triggered=false"
    - "When QUERY_EXPANSION_ENABLED=true and query is weak (< 3 results >= 0.60), HyDE bio is generated and embedding is blended before a second FAISS pass"
    - "When 3 or more results already meet the similarity threshold, HyDE is skipped entirely even if the flag is true"
    - "When FEEDBACK_LEARNING_ENABLED=true, experts with 10+ global feedback interactions and positive ratio receive a proportional boost up to 20% of their original score"
    - "Experts with fewer than 10 global feedback interactions receive zero boost regardless of their vote ratio"
    - "A Feedback DB failure never raises an exception from retrieve_with_intelligence() — it degrades gracefully and returns candidates without re-ranking"
    - "A HyDE Gemini timeout (> 5 seconds) falls back to original candidates silently"
  artifacts:
    - path: "app/services/search_intelligence.py"
      provides: "HyDE + feedback re-ranking wrapper callable from chat.py"
      exports:
        - "retrieve_with_intelligence"
        - "QUERY_EXPANSION_ENABLED"
        - "FEEDBACK_LEARNING_ENABLED"
      min_lines: 120
  key_links:
    - from: "app/services/search_intelligence.py"
      to: "app/services/retriever.py"
      via: "import retrieve, RetrievedExpert, SIMILARITY_THRESHOLD"
      pattern: "from app\\.services\\.retriever import"
    - from: "app/services/search_intelligence.py"
      to: "app/services/embedder.py"
      via: "import embed_query for HyDE bio embedding"
      pattern: "from app\\.services\\.embedder import embed_query"
    - from: "app/services/search_intelligence.py"
      to: "app/models.py"
      via: "import Feedback for vote table query"
      pattern: "from app\\.models import Feedback"
---

<objective>
Create app/services/search_intelligence.py — the complete HyDE query expansion + feedback re-ranking service module.

Purpose: Adds two intelligence layers to the retrieval pipeline, both gated by env var flags. The module is entirely self-contained and does not modify any existing files — chat.py integration happens in Plan 02.

Output: app/services/search_intelligence.py implementing retrieve_with_intelligence(), callable with identical signature shape to retriever.retrieve() plus a db Session argument.
</objective>

<execution_context>
@/Users/sebastianhamers/.claude/get-shit-done/workflows/execute-plan.md
@/Users/sebastianhamers/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/10-search-intelligence-layer/10-CONTEXT.md
@.planning/phases/10-search-intelligence-layer/10-RESEARCH.md
@app/services/retriever.py
@app/services/embedder.py
@app/models.py
@app/config.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create search_intelligence.py service module</name>
  <files>app/services/search_intelligence.py</files>
  <action>
Create app/services/search_intelligence.py implementing the full HyDE + feedback re-ranking pipeline.

**Module-level feature flags (read at import time — Railway injects at startup):**
```python
def _flag(name: str) -> bool:
    return os.getenv(name, "false").lower().strip() in ("true", "1", "yes")

QUERY_EXPANSION_ENABLED: bool = _flag("QUERY_EXPANSION_ENABLED")
FEEDBACK_LEARNING_ENABLED: bool = _flag("FEEDBACK_LEARNING_ENABLED")
```
Default both to False — Railway explicitly enables them after validation.

**Constants:**
- `STRONG_RESULT_MIN = 3` — skip HyDE if >= 3 candidates already have score >= SIMILARITY_THRESHOLD
- `HYDE_TIMEOUT_SECONDS = 5.0` — abort HyDE LLM call after 5 seconds (known gemini-2.5-flash socket hang bug — see RESEARCH.md Pitfall 4)

**Public API — the only function chat.py will call:**
```python
def retrieve_with_intelligence(
    query: str,
    faiss_index,
    metadata: list[dict],
    db: Session,
) -> tuple[list[RetrievedExpert], dict]:
```
Returns `(candidates, intelligence_meta)` where intelligence_meta is always:
`{"hyde_triggered": bool, "hyde_bio": str | None, "feedback_applied": bool}`

**Internal flow of retrieve_with_intelligence:**
1. Call `retrieve(query, faiss_index, metadata)` for initial candidates (import from retriever.py)
2. If `QUERY_EXPANSION_ENABLED` AND `_is_weak_query(candidates)` is True:
   a. Call `_generate_hypothetical_bio(query)` — returns `str | None`
   b. If bio is not None: embed original query with `embed_query(query)`, blend with `_blend_embeddings()`, re-search FAISS with `_search_with_vector()`, merge with `_merge_candidates()`
   c. Set `intelligence["hyde_triggered"] = True`, `intelligence["hyde_bio"] = bio`
   d. Log: `log.info("hyde.triggered", query_preview=query[:60])`
3. If `FEEDBACK_LEARNING_ENABLED`:
   a. Call `_apply_feedback_boost(candidates, db)`
   b. Set `intelligence["feedback_applied"] = True`
4. Return `(candidates, intelligence)`

NOTE: retrieve_with_intelligence is synchronous (runs in thread pool from chat.py). It MUST NOT be async — it calls synchronous genai.Client() and embed_query(). The asyncio.wait_for timeout for HyDE is handled in chat.py's run_in_executor wrapper (Plan 02 concern). Document this in the module docstring.

**_is_weak_query(candidates):**
```python
def _is_weak_query(candidates: list[RetrievedExpert]) -> bool:
    strong = sum(1 for c in candidates if c.score >= SIMILARITY_THRESHOLD)
    return strong < STRONG_RESULT_MIN
```

**_generate_hypothetical_bio(query) — lazy singleton genai.Client():**
Use the same lazy singleton pattern as embedder.py (`_hyde_client: genai.Client | None = None`, `_get_hyde_client()` function). Use `genai.Client()` (sync client — NOT `client.aio`).

Call `_get_hyde_client().models.generate_content()` with model `"gemini-2.5-flash"` and config:
```python
types.GenerateContentConfig(temperature=0.7, max_output_tokens=200)
```

Prompt (bio-shaped, first-person, domain-focused — must match expert embedding space):
```
Write a short professional bio (2-3 sentences) for an expert consultant who would be the perfect answer to this problem:

"{query}"

Write the bio in first person. Focus on domain expertise, not generic skills. Example style: 'I am a tax attorney specializing in EU VAT compliance for e-commerce companies. I have advised 50+ startups on cross-border tax structures.'
```

Wrap the entire call in try/except Exception. On any exception: `log.warning("hyde.generation_failed", error=str(exc)); return None`.
Strip whitespace from response.text; return None if empty string.

**_blend_embeddings(original_vec, hyde_text) — CRITICAL normalization:**
```python
def _blend_embeddings(original_vec: list[float], hyde_text: str) -> list[float]:
    hyde_vec = embed_query(hyde_text)  # Already L2-normalized by embed_query()
    orig = np.array(original_vec, dtype=np.float32)
    hyp = np.array(hyde_vec, dtype=np.float32)
    blended = (orig + hyp) / 2.0
    blended = blended.reshape(1, -1)
    faiss.normalize_L2(blended)  # MANDATORY — averaged vectors are NOT unit length
    return blended[0].tolist()
```
NEVER skip the faiss.normalize_L2 call — without it IndexFlatIP scores are corrupted (RESEARCH.md Pitfall 1).

**_search_with_vector(blended_vec, faiss_index, metadata) — re-search FAISS:**
Replicate the search pattern from retriever.retrieve() but accept a pre-built vector instead of a query string. Use TOP_K=5 (import from retriever). Return list[RetrievedExpert]. Import `TOP_K` from `app.services.retriever`.

**_merge_candidates(original, hyde_candidates) — deduplicate by profile_url:**
Merge original and hyde_candidates into one list. Deduplicate keeping the higher score for any profile_url seen twice. Re-sort descending by score. Return up to TOP_K results. Experts with `profile_url=None` are deduped by name instead (use expert name as fallback key).

**_apply_feedback_boost(candidates, db) — feedback re-ranking:**
Implement using the exact pattern from RESEARCH.md Pattern 5.

Key rules:
- Cold-start guard: skip experts with total (up + down) < 10 interactions (SEARCH-05)
- Boost formula: `ratio = up / total`; if `ratio > 0.5`: `boost = (ratio - 0.5) * 0.4` → multiplier = `1.0 + boost` (max 1.20); if `ratio < 0.5`: `penalty = (0.5 - ratio) * 0.4` → multiplier = `1.0 - penalty` (min 0.80)
- Apply multiplier: `candidate.score *= multiplier` — but RetrievedExpert is a dataclass, so create new instances or update score in-place (dataclass fields are mutable)
- Re-sort by score descending after applying all multipliers
- Wrap ENTIRE function in try/except Exception; on failure: `log.warning("feedback.score_load_failed", error=str(exc)); return candidates` (NEVER raise — SEARCH-06 requires graceful degradation)
- Load all Feedback rows filtered to `vote.in_(["up", "down"])` then parse expert_ids JSON in Python (SQLite has no JSON indexing)
- Use `url_set = set(urls)` guard — return {} immediately if empty set (avoid empty .in_() query, same pattern as Phase 09-01 decision in STATE.md)

**Imports section:**
```python
import os
import json
import numpy as np
import faiss
import structlog

from google import genai
from google.genai import types
from sqlalchemy.orm import Session
from sqlalchemy import select

from app.services.embedder import embed_query
from app.services.retriever import retrieve, RetrievedExpert, SIMILARITY_THRESHOLD, TOP_K
from app.models import Feedback
```

Do NOT import `app.config.OUTPUT_DIM` — it's not needed (embed_query handles dimensions).

**Module docstring** must explain: both flags default False, retrieve_with_intelligence is sync (run via run_in_executor from chat.py), and the HyDE timeout is handled by the caller.
  </action>
  <verify>
Run from /Users/sebastianhamers/Documents/TCS:
```
python -c "from app.services.search_intelligence import retrieve_with_intelligence, QUERY_EXPANSION_ENABLED, FEEDBACK_LEARNING_ENABLED; print('Import OK', QUERY_EXPANSION_ENABLED, FEEDBACK_LEARNING_ENABLED)"
```
Expected output: `Import OK False False`

Also verify the module has no syntax errors:
```
python -m py_compile app/services/search_intelligence.py && echo "Syntax OK"
```
  </verify>
  <done>
`app/services/search_intelligence.py` imports cleanly with both flags defaulting to False. `retrieve_with_intelligence` is importable and callable. No syntax errors. All internal functions (_is_weak_query, _generate_hypothetical_bio, _blend_embeddings, _search_with_vector, _merge_candidates, _apply_feedback_boost) are defined and called from retrieve_with_intelligence.
  </done>
</task>

</tasks>

<verification>
1. `python -c "from app.services.search_intelligence import retrieve_with_intelligence, QUERY_EXPANSION_ENABLED, FEEDBACK_LEARNING_ENABLED; print('Import OK', QUERY_EXPANSION_ENABLED, FEEDBACK_LEARNING_ENABLED)"` prints `Import OK False False`
2. `python -m py_compile app/services/search_intelligence.py && echo "Syntax OK"` passes
3. File exports `retrieve_with_intelligence`, `QUERY_EXPANSION_ENABLED`, `FEEDBACK_LEARNING_ENABLED`
4. `grep "QUERY_EXPANSION_ENABLED\|FEEDBACK_LEARNING_ENABLED" app/services/search_intelligence.py` shows both flags with `_flag()` function
5. `grep "normalize_L2" app/services/search_intelligence.py` confirms L2 normalization after blend
6. `grep "try:\|except Exception" app/services/search_intelligence.py` shows both HyDE and feedback wrapped in try/except
</verification>

<success_criteria>
- app/services/search_intelligence.py exists with all 6 internal functions implemented
- Both feature flags default to False (os.getenv default is "false")
- retrieve_with_intelligence signature: (query, faiss_index, metadata, db) -> tuple[list[RetrievedExpert], dict]
- intelligence_meta dict always has keys: hyde_triggered, hyde_bio, feedback_applied
- HyDE path: only fires when QUERY_EXPANSION_ENABLED=true AND fewer than 3 results >= SIMILARITY_THRESHOLD
- Feedback path: only fires when FEEDBACK_LEARNING_ENABLED=true; cold-start guard enforced at total < 10
- Both failure paths degrade gracefully (return candidates unchanged, never raise)
</success_criteria>

<output>
After completion, create `.planning/phases/10-search-intelligence-layer/10-01-SUMMARY.md`
</output>
