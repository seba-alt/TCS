---
phase: 01-foundation
plan: 02
type: execute
wave: 2
depends_on:
  - "01-01"
files_modified:
  - app/__init__.py
  - app/config.py
  - app/services/__init__.py
  - app/services/embedder.py
  - scripts/ingest.py
autonomous: true
requirements:
  - REC-01
user_setup:
  - service: google-genai
    why: "Embedding API calls during ingest.py and at query time"
    env_vars:
      - name: GOOGLE_API_KEY
        source: "Google AI Studio -> Get API Key (https://aistudio.google.com/)"
    dashboard_config: []

must_haves:
  truths:
    - "A direct Python call `from app.services.embedder import embed_query; v = embed_query('test'); assert len(v) == 768` succeeds without error"
    - "Running `python scripts/ingest.py` with a valid `data/experts.csv` produces `data/faiss.index` and `data/metadata.json` on disk without errors"
    - "The FAISS index contains the expected number of vectors (matches CSV row count)"
    - "`GOOGLE_API_KEY` is read from environment (`.env` file or shell) — never hardcoded in any source file"
  artifacts:
    - path: "app/config.py"
      provides: "Single source of truth for OUTPUT_DIM=768 and model name constants"
      contains: "OUTPUT_DIM"
    - path: "app/services/embedder.py"
      provides: "embed_query() for runtime query embedding — reused by Phase 2 retriever"
      exports: ["embed_query"]
      min_lines: 25
    - path: "scripts/ingest.py"
      provides: "Offline ingestion: CSV → embeddings → FAISS index + metadata JSON"
      min_lines: 80
    - path: "data/faiss.index"
      provides: "Binary FAISS index on disk (gitignored, generated by ingest.py)"
    - path: "data/metadata.json"
      provides: "Position-aligned metadata array for FAISS results (gitignored)"
  key_links:
    - from: "scripts/ingest.py"
      to: "app/services/embedder.py"
      via: "imports embed_batch (or uses same google-genai client pattern)"
      pattern: "from app.services.embedder|from google import genai"
    - from: "scripts/ingest.py"
      to: "data/faiss.index"
      via: "faiss.write_index()"
      pattern: "faiss\\.write_index"
    - from: "app/services/embedder.py"
      to: "app/config.py"
      via: "imports OUTPUT_DIM and EMBEDDING_MODEL"
      pattern: "from app.config import|OUTPUT_DIM"
---

<objective>
Build the embedder service and offline ingestion pipeline that pre-computes expert embeddings and writes the FAISS index to disk.

Purpose: The FAISS index is the hard dependency for the FastAPI server. Without it, Phase 2 cannot do retrieval and Phase 3 has nothing to serve. This plan creates the one-off offline script that builds the index and the reusable embedder service that Phase 2 will import.

Output: `app/config.py` (constants), `app/services/embedder.py` (reusable embed_query), `scripts/ingest.py` (offline CSV → FAISS), and the resulting `data/faiss.index` + `data/metadata.json` on disk.
</objective>

<execution_context>
@/Users/sebastianhamers/.claude/get-shit-done/workflows/execute-plan.md
@/Users/sebastianhamers/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/01-foundation/1-RESEARCH.md
@.planning/phases/01-foundation/01-01-SUMMARY.md
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create app/config.py and app/services/embedder.py</name>
  <files>app/__init__.py, app/config.py, app/services/__init__.py, app/services/embedder.py</files>
  <action>
First, create the package init files:
- `app/__init__.py` — empty file
- `app/services/__init__.py` — empty file

Create `app/config.py` — single source of truth for constants shared across ingest and runtime:

```python
"""
Shared configuration constants.

OUTPUT_DIM must match between ingest.py (build-time) and embedder.py (runtime).
Changing OUTPUT_DIM requires re-running ingest.py to rebuild the FAISS index.
"""
from pathlib import Path

# Embedding model — text-embedding-004 is SHUT DOWN (Jan 14, 2026). Do not use.
EMBEDDING_MODEL = "gemini-embedding-001"

# Truncate from 3072 to 768 dims: 75% storage savings, 0.26% quality loss.
# IMPORTANT: 768-dim vectors are NOT pre-normalized. ingest.py and embedder.py
# must both call faiss.normalize_L2() before FAISS operations.
OUTPUT_DIM = 768

# Batch size for embedding API calls during ingestion.
# 100 is conservative — backoff handles rate limit 429s.
INGEST_BATCH_SIZE = 100

# Absolute paths to data files — avoids CWD-relative path failures.
DATA_DIR = Path(__file__).resolve().parent.parent / "data"
FAISS_INDEX_PATH = DATA_DIR / "faiss.index"
METADATA_PATH = DATA_DIR / "metadata.json"
```

Create `app/services/embedder.py` — wraps google-genai for single-query embedding (used at runtime by Phase 2 retriever):

```python
"""
Embedder service — wraps google-genai for query-time embedding.

At build time (ingest.py), embeddings are batch-computed separately.
At runtime, this module embeds one query at a time for retrieval.

Task type asymmetry (IMPORTANT):
  - Indexing: task_type="RETRIEVAL_DOCUMENT"
  - Querying: task_type="RETRIEVAL_QUERY"
Using the wrong task_type degrades retrieval quality.
"""
import numpy as np
from google import genai
from google.genai import types

from app.config import EMBEDDING_MODEL, OUTPUT_DIM

# Client picks up GOOGLE_API_KEY (or GEMINI_API_KEY) from environment.
# Never pass the key as a constructor argument.
_client = genai.Client()


def embed_query(text: str) -> list[float]:
    """
    Embed a single query string for semantic search.

    Returns a list of OUTPUT_DIM floats, L2-normalized for cosine similarity
    via FAISS IndexFlatIP.

    Args:
        text: The user's natural language query.

    Returns:
        list[float] of length OUTPUT_DIM (768).
    """
    result = _client.models.embed_content(
        model=EMBEDDING_MODEL,
        contents=text,
        config=types.EmbedContentConfig(
            task_type="RETRIEVAL_QUERY",
            output_dimensionality=OUTPUT_DIM,
        ),
    )
    vector = np.array(result.embeddings[0].values, dtype=np.float32).reshape(1, -1)
    # Normalize: truncated-dim vectors are NOT pre-normalized by Google.
    import faiss
    faiss.normalize_L2(vector)
    return vector[0].tolist()
```

IMPORTANT pitfall to avoid in this file:
- Do NOT hardcode `GOOGLE_API_KEY` in source. `genai.Client()` reads from environment automatically.
- Always import OUTPUT_DIM from `app.config`, never hardcode 768 inline.
  </action>
  <verify>
1. `python -m py_compile app/config.py && echo "config OK"`
2. `python -m py_compile app/services/embedder.py && echo "embedder OK"`
3. `python -c "from app.config import OUTPUT_DIM, EMBEDDING_MODEL; assert OUTPUT_DIM == 768; assert EMBEDDING_MODEL == 'gemini-embedding-001'; print('config values OK')"` — must pass
4. `python -c "from app.services.embedder import embed_query; print('import OK')"` — must import without error (no API call needed for import check)
  </verify>
  <done>`app/config.py` exports `OUTPUT_DIM=768` and `EMBEDDING_MODEL="gemini-embedding-001"`. `app/services/embedder.py` exports `embed_query(text: str) -> list[float]` and imports constants from config (not hardcoded). Both files have valid Python syntax.</done>
</task>

<task type="auto">
  <name>Task 2: Create ingest.py and run ingestion</name>
  <files>scripts/ingest.py</files>
  <action>
Create `scripts/ingest.py`. This is a one-off offline script — it MUST NOT be called at API startup.

CRITICAL before writing this script: Read the output of `scripts/validate_csv.py` from Plan 01 (if already run against `data/experts.csv`) to confirm the actual column names. The script below uses common column name candidates — adjust `expert_to_text()` to match the real CSV columns after validation.

```python
#!/usr/bin/env python3
"""
Offline ingestion: experts.csv → FAISS index + metadata JSON.

Run ONCE before starting the API server:
  python scripts/ingest.py

NEVER call this at API startup — it takes 60+ seconds and hits the embedding API.

Prerequisites:
  1. GOOGLE_API_KEY set in environment (or .env file)
  2. data/experts.csv exists
  3. Run scripts/validate_csv.py first to confirm column names
"""
import json
import sys
import time
from pathlib import Path

import faiss
import numpy as np
import pandas as pd
from dotenv import load_dotenv
from google import genai
from google.genai import types
from tenacity import retry, stop_after_attempt, wait_exponential

# Load .env for local development — no-op in production
load_dotenv()

# Import shared constants — OUTPUT_DIM must match embedder.py
sys.path.insert(0, str(Path(__file__).resolve().parent.parent))
from app.config import (
    EMBEDDING_MODEL,
    FAISS_INDEX_PATH,
    INGEST_BATCH_SIZE,
    METADATA_PATH,
    OUTPUT_DIM,
)

client = genai.Client()


def expert_to_text(row: dict) -> str:
    """
    Construct semantically rich embedding text from a CSV row.

    IMPORTANT: Adjust field names below to match your actual CSV columns.
    Run scripts/validate_csv.py first to see the exact column names.

    Field weight rationale:
    - title/role carries the most retrieval signal
    - company provides industry/context signal
    - bio is truncated to 300 chars to prevent long bios dominating the vector
    """
    # Common column name candidates — update after running validate_csv.py
    title = row.get("title") or row.get("job_title") or row.get("role") or ""
    company = row.get("company") or row.get("organization") or row.get("employer") or ""
    industry = row.get("industry") or row.get("sector") or row.get("category") or ""
    bio = row.get("bio") or row.get("description") or row.get("about") or row.get("summary") or ""

    bio_truncated = str(bio)[:300] if bio else ""

    parts = []
    if title:
        parts.append(f"{title} at {company}." if company else title + ".")
    if industry:
        parts.append(f"Industry: {industry}.")
    if bio_truncated:
        parts.append(bio_truncated)

    return " ".join(parts) if parts else str(row)


@retry(
    stop=stop_after_attempt(5),
    wait=wait_exponential(multiplier=1, min=4, max=60),
    reraise=True,
)
def embed_batch(texts: list[str]) -> list[list[float]]:
    """
    Embed a batch of texts with tenacity retry for 429 rate limit errors.
    Returns list of OUTPUT_DIM-length float vectors.
    """
    result = client.models.embed_content(
        model=EMBEDDING_MODEL,
        contents=texts,
        config=types.EmbedContentConfig(
            task_type="RETRIEVAL_DOCUMENT",  # Different from RETRIEVAL_QUERY at runtime
            output_dimensionality=OUTPUT_DIM,
        ),
    )
    return [e.values for e in result.embeddings]


def build_index(df: pd.DataFrame) -> tuple[faiss.IndexFlatIP, list[dict]]:
    """
    Embed all experts in batches and build a FAISS IndexFlatIP.
    Applies L2 normalization (required for truncated-dim cosine similarity).
    """
    all_vectors: list[list[float]] = []
    metadata: list[dict] = []
    total = len(df)

    for i in range(0, total, INGEST_BATCH_SIZE):
        batch = df.iloc[i:i + INGEST_BATCH_SIZE]
        texts = [expert_to_text(row.to_dict()) for _, row in batch.iterrows()]

        try:
            vectors = embed_batch(texts)
        except Exception as e:
            print(f"[error] Batch {i}-{i + len(batch)} failed after retries: {e}")
            raise

        all_vectors.extend(vectors)
        for _, row in batch.iterrows():
            metadata.append(row.to_dict())

        done = min(i + INGEST_BATCH_SIZE, total)
        print(f"  Embedded {done}/{total} experts ({done * 100 // total}%)")

        # Throttle slightly to stay within rate limits
        if i + INGEST_BATCH_SIZE < total:
            time.sleep(0.5)

    # L2 normalize: REQUIRED for cosine similarity via IndexFlatIP on truncated dims.
    # Full 3072-dim vectors are pre-normalized; 768-dim vectors are NOT.
    matrix = np.array(all_vectors, dtype=np.float32)
    faiss.normalize_L2(matrix)

    index = faiss.IndexFlatIP(OUTPUT_DIM)
    index.add(matrix)

    return index, metadata


def main() -> None:
    csv_path = Path(sys.argv[1]) if len(sys.argv) > 1 else Path("data/experts.csv")

    if not csv_path.exists():
        print(f"[error] CSV not found: {csv_path}")
        print("  Place experts.csv in data/ and run: python scripts/validate_csv.py first")
        sys.exit(1)

    print(f"Loading CSV: {csv_path}")
    try:
        df = pd.read_csv(csv_path, encoding="utf-8-sig")
    except UnicodeDecodeError:
        df = pd.read_csv(csv_path, encoding="latin-1")

    print(f"  {len(df)} experts loaded, {len(df.columns)} columns")
    print(f"  Columns: {list(df.columns)}")
    print()

    print(f"Embedding {len(df)} experts in batches of {INGEST_BATCH_SIZE}...")
    print(f"  Model: {EMBEDDING_MODEL}, dim: {OUTPUT_DIM}")
    print()

    index, metadata = build_index(df)

    # Ensure data directory exists
    FAISS_INDEX_PATH.parent.mkdir(parents=True, exist_ok=True)

    print()
    print("Writing index to disk...")
    faiss.write_index(index, str(FAISS_INDEX_PATH))
    print(f"  FAISS index: {FAISS_INDEX_PATH} ({index.ntotal} vectors)")

    with open(METADATA_PATH, "w", encoding="utf-8") as f:
        json.dump(metadata, f, ensure_ascii=False, indent=None, default=str)
    print(f"  Metadata:    {METADATA_PATH} ({len(metadata)} records)")

    print()
    print(f"Ingestion complete: {index.ntotal} experts indexed at {OUTPUT_DIM} dims.")


if __name__ == "__main__":
    main()
```

After creating the file:
1. Verify Python syntax compiles cleanly
2. If `data/experts.csv` exists AND `GOOGLE_API_KEY` is set in the environment: run `python scripts/ingest.py` to produce the actual index. This may take several minutes due to API batching.
3. If `data/experts.csv` does not exist yet: syntax verification is sufficient — the index will be built when the CSV is provided.

After ingest (if CSV is available): verify `data/faiss.index` and `data/metadata.json` exist and are non-empty.
  </action>
  <verify>
1. `python -m py_compile scripts/ingest.py && echo "syntax OK"`
2. If CSV is available: `python scripts/ingest.py && ls -la data/faiss.index data/metadata.json` — both files must exist with non-zero size
3. If CSV is available: `python -c "import faiss; idx = faiss.read_index('data/faiss.index'); print(f'Index OK: {idx.ntotal} vectors, dim={idx.d}'); assert idx.d == 768"` — must print vector count and confirm dim=768
4. `git status data/` — `data/faiss.index`, `data/metadata.json`, `data/experts.csv` must NOT appear (gitignored)
  </verify>
  <done>`scripts/ingest.py` has valid Python syntax. If CSV is present: `data/faiss.index` and `data/metadata.json` exist on disk, `faiss.read_index()` loads successfully and confirms dim=768. If CSV is not yet present: syntax verification passes and the script is ready to run when the CSV arrives.</done>
</task>

</tasks>

<verification>
1. `python -c "from app.config import OUTPUT_DIM, EMBEDDING_MODEL; assert OUTPUT_DIM == 768; print('constants OK')"` — passes
2. `python -c "from app.services.embedder import embed_query; print('embedder importable')"` — no import error
3. If GOOGLE_API_KEY is set: `python -c "from app.services.embedder import embed_query; v = embed_query('test query'); assert len(v) == 768; print(f'embed_query OK: {len(v)} dims')"` — returns 768-dim vector (Success Criterion 4)
4. If ingest ran: `python -c "import faiss; idx = faiss.read_index('data/faiss.index'); assert idx.d == 768; print(f'FAISS OK: {idx.ntotal} vectors')"` — confirms index is valid
</verification>

<success_criteria>
- `app/config.py` defines `OUTPUT_DIM=768` and `EMBEDDING_MODEL="gemini-embedding-001"` as single source of truth
- `app/services/embedder.py` exports `embed_query(text: str) -> list[float]` returning 768-dim L2-normalized vector
- `scripts/ingest.py` runs without errors when given `data/experts.csv` and produces `data/faiss.index` + `data/metadata.json`
- Phase success criterion 4 passes: direct Python call to `embed_query` returns 768-dim vector
- No source file contains a hardcoded API key
</success_criteria>

<output>
After completion, create `.planning/phases/01-foundation/01-02-SUMMARY.md`
</output>
