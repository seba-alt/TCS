---
phase: 01-foundation
plan: 01
type: execute
wave: 1
depends_on: []
files_modified:
  - .gitignore
  - requirements.txt
  - scripts/validate_csv.py
  - data/.gitkeep
autonomous: true
requirements:
  - REC-01

must_haves:
  truths:
    - "Running `python scripts/validate_csv.py` prints column names, row count, and flags any missing required fields — no crash"
    - "`.env` is gitignored: `git status` does not show `.env` as a tracked or untracked file after it is created"
    - "`data/faiss.index`, `data/metadata.json`, `data/experts.csv` are gitignored: none appear in `git status` after being created"
    - "All required Python packages install cleanly from `requirements.txt` with no version conflicts"
  artifacts:
    - path: ".gitignore"
      provides: "Prevents secrets and generated data files from entering git history"
      contains: ".env"
    - path: "requirements.txt"
      provides: "Pinned dependency list for reproducible installs"
      contains: "google-genai"
    - path: "scripts/validate_csv.py"
      provides: "CSV quality gate — first action before any embedding work"
      min_lines: 40
  key_links:
    - from: ".gitignore"
      to: ".env"
      via: "gitignore pattern"
      pattern: "\\.env"
    - from: ".gitignore"
      to: "data/"
      via: "gitignore pattern"
      pattern: "data/faiss\\.index"
---

<objective>
Bootstrap the project scaffold: gitignore, pinned dependencies, and a CSV validation script that runs before any embedding work begins.

Purpose: Secrets and generated data files must be excluded from git before a single line of application code is written. The CSV column names and data quality are unknown until `validate_csv.py` runs — this blocks the field names used in `ingest.py`.

Output: `.gitignore`, `requirements.txt`, `scripts/validate_csv.py`, `data/` directory placeholder.
</objective>

<execution_context>
@/Users/sebastianhamers/.claude/get-shit-done/workflows/execute-plan.md
@/Users/sebastianhamers/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/01-foundation/1-RESEARCH.md
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create .gitignore and requirements.txt</name>
  <files>.gitignore, requirements.txt</files>
  <action>
Create `.gitignore` with the following entries (this file must be committed FIRST, before any other files, to prevent secrets entering git history):

```
# Secrets — never commit
.env
.env.*
.env.local

# Generated data files — gitignored because they are large and regenerable
data/faiss.index
data/metadata.json
data/experts.csv

# Python
__pycache__/
*.py[cod]
.venv/
*.egg-info/

# OS
.DS_Store
Thumbs.db
```

Create `requirements.txt` with these exact pinned versions (from research confirming PyPI as of 2026-02-20):

```
# Core backend
fastapi[standard]==0.129.*
uvicorn[standard]==0.29.*
pydantic==2.7.*
google-genai==1.64.*
faiss-cpu==1.13.*
pandas==2.2.*
numpy==1.26.*
python-dotenv==1.0.*
tenacity==8.4.*
structlog==24.2.*

# Dev / testing
pytest==8.2.*
pytest-asyncio==0.23.*
httpx==0.27.*
ruff
```

DO NOT use `google-generativeai` — it is the deprecated SDK. The active SDK is `google-genai` (import path: `from google import genai`).

Create the `data/` directory with a `.gitkeep` placeholder so the directory exists in git:
- `data/.gitkeep` — empty file

Create `scripts/` directory.

After writing both files, verify the gitignore is working by creating a test `.env` file and confirming `git status` does not show it as an untracked file.
  </action>
  <verify>
1. `cat .gitignore | grep -E "\.env|faiss\.index|experts\.csv"` — must show all three patterns
2. `echo "GOOGLE_API_KEY=test" > .env && git status` — `.env` must NOT appear in output (shows as ignored)
3. `cat requirements.txt | grep "google-genai"` — must show `google-genai==1.64.*` (NOT `google-generativeai`)
4. `rm .env` — clean up test file
  </verify>
  <done>`.gitignore` excludes `.env`, `data/faiss.index`, `data/metadata.json`, `data/experts.csv`. `requirements.txt` pins all dependencies with `google-genai` (not `google-generativeai`). Confirmed via git status that `.env` is invisible to git.</done>
</task>

<task type="auto">
  <name>Task 2: Create CSV validation script</name>
  <files>scripts/validate_csv.py</files>
  <action>
Create `scripts/validate_csv.py`. This script is the FIRST thing run in Phase 1 before any embedding work — it reveals the actual CSV column names and data quality.

The script must:

1. Accept the CSV path as a CLI argument (default: `data/experts.csv`)
2. Load the CSV using `pd.read_csv(path, encoding="utf-8-sig")` — `utf-8-sig` handles BOM characters common in Excel exports. If this raises `UnicodeDecodeError`, retry with `chardet` to detect encoding.
3. Print a discovery report:
   - Column names (exact strings, so field names are known for `ingest.py`)
   - Row count
   - Sample of 3 rows (first 3, printed as dict)
4. Assert required columns exist. Required columns (assumed from REQUIREMENTS.md + project context — may need adjustment after first run):
   - Must have at minimum: some name column, some title/role column, some company column, some URL column
   - Print a WARNING (not error) if `bio`, `hourly_rate`, or similar descriptive columns are missing — ingestion can still work without them, but retrieval quality degrades
5. Report data quality issues:
   - Count of rows where URL-like columns are empty or null
   - Count of rows where name column is null
   - Count of rows with empty bio (if bio column exists)
6. Exit with code 0 if no critical errors (no required columns missing), exit 1 if critical columns are absent

Full implementation:

```python
#!/usr/bin/env python3
"""
Validate experts.csv before ingestion.

Run: python scripts/validate_csv.py [path/to/experts.csv]
     Default path: data/experts.csv
"""
import sys
from pathlib import Path

import pandas as pd


def detect_encoding(path: Path) -> str:
    try:
        import chardet
        raw = path.read_bytes()
        result = chardet.detect(raw[:10000])
        return result.get("encoding", "utf-8") or "utf-8"
    except ImportError:
        return "utf-8"


def load_csv(path: Path) -> pd.DataFrame:
    try:
        return pd.read_csv(path, encoding="utf-8-sig")
    except UnicodeDecodeError:
        enc = detect_encoding(path)
        print(f"[warn] UTF-8 decode failed; retrying with detected encoding: {enc}")
        return pd.read_csv(path, encoding=enc)


def validate(csv_path: Path) -> bool:
    if not csv_path.exists():
        print(f"[error] File not found: {csv_path}")
        return False

    df = load_csv(csv_path)

    print("=" * 60)
    print("CSV DISCOVERY REPORT")
    print("=" * 60)
    print(f"Rows: {len(df)}")
    print(f"Columns ({len(df.columns)}): {list(df.columns)}")
    print()
    print("Sample (first 3 rows):")
    for i, row in df.head(3).iterrows():
        print(f"  Row {i}: {row.to_dict()}")
    print()

    cols_lower = {c.lower(): c for c in df.columns}

    # --- Data quality checks ---
    warnings = []
    errors = []

    # Check for URL column
    url_candidates = [c for c in cols_lower if "url" in c or "link" in c or "profile" in c]
    if url_candidates:
        url_col = cols_lower[url_candidates[0]]
        null_urls = df[url_col].isna().sum()
        empty_urls = (df[url_col].astype(str).str.strip() == "").sum()
        bad_urls = null_urls + empty_urls
        if bad_urls > 0:
            warnings.append(f"URL column '{url_col}': {bad_urls} empty/null values")
    else:
        errors.append("No URL column detected (expected column containing 'url', 'link', or 'profile')")

    # Check for name column
    name_candidates = [c for c in cols_lower if "name" in c]
    if name_candidates:
        name_col = cols_lower[name_candidates[0]]
        null_names = df[name_col].isna().sum()
        if null_names > 0:
            warnings.append(f"Name column '{name_col}': {null_names} null values")
    else:
        errors.append("No name column detected (expected column containing 'name')")

    # Check for bio/description column
    bio_candidates = [c for c in cols_lower if any(k in c for k in ("bio", "description", "about", "summary"))]
    if bio_candidates:
        bio_col = cols_lower[bio_candidates[0]]
        empty_bios = df[bio_col].isna().sum() + (df[bio_col].astype(str).str.strip() == "").sum()
        if empty_bios > 0:
            warnings.append(f"Bio column '{bio_col}': {empty_bios} empty/null (retrieval quality may degrade)")
    else:
        warnings.append("No bio/description column detected — embedding text will be title + company only")

    # Check for rate column
    rate_candidates = [c for c in cols_lower if any(k in c for k in ("rate", "price", "cost", "fee"))]
    if not rate_candidates:
        warnings.append("No rate/price column detected — hourly rate will not be available in cards")

    # --- Report ---
    if warnings:
        print("WARNINGS:")
        for w in warnings:
            print(f"  [warn] {w}")
        print()

    if errors:
        print("ERRORS (critical — fix before running ingest.py):")
        for e in errors:
            print(f"  [error] {e}")
        print()
        return False

    print("STATUS: CSV is valid — ready for ingest.py")
    print()
    print("NEXT STEP: Update field names in scripts/ingest.py to match columns above.")
    return True


if __name__ == "__main__":
    csv_path = Path(sys.argv[1]) if len(sys.argv) > 1 else Path("data/experts.csv")
    success = validate(csv_path)
    sys.exit(0 if success else 1)
```

This script is intentionally flexible about column names — it reports what it finds rather than asserting exact names, because the actual CSV column names are unknown until the file is inspected.
  </action>
  <verify>
1. `python scripts/validate_csv.py --help || python scripts/validate_csv.py nonexistent.csv; echo "exit: $?"` — running with a nonexistent file should print an error and exit 1
2. `python -c "import scripts.validate_csv" 2>&1 || python -m py_compile scripts/validate_csv.py && echo "syntax OK"` — script must have valid Python syntax
  </verify>
  <done>`scripts/validate_csv.py` exists, runs without syntax errors, exits 1 when CSV file is not found, and would print a discovery report (columns, row count, sample rows) when given a real CSV.</done>
</task>

</tasks>

<verification>
1. `git status` — `.env` and `data/*.index`, `data/*.json`, `data/experts.csv` are all absent from tracked/untracked output
2. `python -m py_compile scripts/validate_csv.py && echo "OK"` — no syntax errors
3. `cat requirements.txt | grep -v "google-generativeai"` — deprecated SDK must NOT appear
4. `cat requirements.txt | grep "google-genai==1.64"` — must confirm pinned version
</verification>

<success_criteria>
- `.gitignore` is in place before any other files are git-tracked
- `data/experts.csv`, `data/faiss.index`, `data/metadata.json`, `.env` are all gitignored
- `requirements.txt` pins all 14 dependencies with `google-genai==1.64.*` (never `google-generativeai`)
- `scripts/validate_csv.py` runs cleanly, exits 1 for missing CSV, exits 0 for valid CSV, prints column names
</success_criteria>

<output>
After completion, create `.planning/phases/01-foundation/01-01-SUMMARY.md`
</output>
