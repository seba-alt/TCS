---
phase: 18-floating-ai-co-pilot
plan: "01"
type: execute
wave: 1
depends_on: []
files_modified:
  - app/services/pilot_service.py
  - app/routers/pilot.py
  - app/main.py
autonomous: true
requirements: [PILOT-02]

must_haves:
  truths:
    - "POST /api/pilot returns JSON with filters (dict or null) and message (string) for any user message"
    - "When a natural language filter request is sent, Gemini extracts apply_filters args and returns them in the filters field"
    - "When no filter intent is detected (greeting, clarification), filters is null and message is Sage's response"
    - "Conversation history is correctly forwarded to Gemini as 'user'/'model' role pairs"
    - "The endpoint uses run_in_executor exactly like explore.py — non-blocking async wrapper"
  artifacts:
    - path: "app/services/pilot_service.py"
      provides: "Two-turn Gemini function calling logic for Sage co-pilot"
      contains: "run_pilot"
    - path: "app/routers/pilot.py"
      provides: "POST /api/pilot endpoint"
      contains: "api/pilot"
    - path: "app/main.py"
      provides: "pilot router registered"
      contains: "pilot"
  key_links:
    - from: "app/routers/pilot.py"
      to: "app/services/pilot_service.py"
      via: "run_pilot function call"
      pattern: "run_pilot"
    - from: "app/main.py"
      to: "app/routers/pilot.py"
      via: "include_router(pilot.router)"
      pattern: "pilot"
    - from: "app/services/pilot_service.py"
      to: "google-genai FunctionDeclaration"
      via: "APPLY_FILTERS_DECLARATION + two-turn generate_content"
      pattern: "apply_filters"
---

<objective>
Build the FastAPI backend pilot service that proxies user messages through Gemini function calling to extract filter intent and generate Sage's confirmation response.

Purpose: The co-pilot needs a backend because the Gemini API key must stay server-side. This service makes two Gemini calls per user message: Turn 1 extracts the `apply_filters` function call with structured filter args, Turn 2 sends the function result back to get Sage's natural language confirmation. The frontend dispatches the returned filters to Zustand and triggers a re-fetch.

Output: `pilot_service.py` (two-turn logic), `pilot.py` router (POST /api/pilot), and main.py updated to register the router.
</objective>

<execution_context>
@/Users/sebastianhamers/.claude/get-shit-done/workflows/execute-plan.md
@/Users/sebastianhamers/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/STATE.md
@.planning/phases/18-floating-ai-co-pilot/18-RESEARCH.md
@app/routers/explore.py
@app/services/llm.py
@app/main.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create pilot_service.py with two-turn Gemini function calling</name>
  <files>app/services/pilot_service.py</files>
  <action>
Create app/services/pilot_service.py implementing the two-turn Gemini function calling proxy for Sage.

Use the same lazy-init `_get_client()` pattern as llm.py and embedder.py. google-genai==1.64 is already installed.

```python
"""
Pilot service — Gemini function calling proxy for the Sage co-pilot.

Two-turn pattern:
  Turn 1: Send user message + apply_filters FunctionDeclaration → Gemini returns function call
  Turn 2: Send function result back → Gemini generates Sage's confirmation response

The browser owns filter state. This service only extracts intent and generates confirmations.
"""
from __future__ import annotations

import structlog
from google import genai
from google.genai import types

log = structlog.get_logger()

GENERATION_MODEL = "gemini-2.5-flash"

APPLY_FILTERS_DECLARATION = types.FunctionDeclaration(
    name="apply_filters",
    description="Update the expert marketplace search filters based on user request.",
    parameters_json_schema={
        "type": "object",
        "properties": {
            "query": {
                "type": "string",
                "description": "Text search query. Empty string to clear the search.",
            },
            "rate_min": {
                "type": "number",
                "description": "Minimum hourly rate filter.",
            },
            "rate_max": {
                "type": "number",
                "description": "Maximum hourly rate filter.",
            },
            "tags": {
                "type": "array",
                "items": {"type": "string"},
                "description": "Domain tags to filter by (AND logic). Empty array clears tag filters.",
            },
            "reset": {
                "type": "boolean",
                "description": "If true, clear all filters and show all experts.",
            },
        },
        "required": [],
    },
)

_client: genai.Client | None = None


def _get_client() -> genai.Client:
    global _client
    if _client is None:
        _client = genai.Client()
    return _client


def run_pilot(
    message: str,
    history: list[dict],
    current_filters: dict,
) -> dict:
    """
    Two-turn Gemini function calling proxy for Sage.

    Args:
        message: Current user message.
        history: Prior conversation as list of {"role": "user"|"model", "content": str}.
                 IMPORTANT: role must be 'user' or 'model' (not 'assistant').
        current_filters: Active filter state {query, rate_min, rate_max, tags}.

    Returns:
        {"filters": dict | None, "message": str}
        - filters: apply_filters args if Gemini called the function, else None
        - message: Sage's natural language response
    """
    client = _get_client()
    tool = types.Tool(function_declarations=[APPLY_FILTERS_DECLARATION])

    system_instruction = (
        "You are Sage, a warm and helpful AI assistant for a professional expert marketplace. "
        "Help users find the right experts by updating search filters when they describe what they need. "
        "Be conversational, friendly, and concise. When a user describes a type of expert or budget, "
        "call apply_filters with the appropriate parameters. Follow-up messages should layer on top of "
        "current filters unless the user asks to reset or start over. "
        "If unsure, ask a clarifying question rather than guessing. "
        f"Current active filters: {current_filters}."
    )

    config = types.GenerateContentConfig(
        tools=[tool],
        system_instruction=system_instruction,
    )

    # Build conversation history as Content objects
    # NOTE: Gemini uses 'user' and 'model' roles (not 'assistant')
    contents: list[types.Content] = []
    for h in history:
        contents.append(types.Content(
            role=h["role"],  # must be 'user' or 'model'
            parts=[types.Part(text=h["content"])]
        ))
    contents.append(types.Content(
        role="user",
        parts=[types.Part(text=message)]
    ))

    # Turn 1: Attempt to extract apply_filters function call
    try:
        response = client.models.generate_content(
            model=GENERATION_MODEL,
            contents=contents,
            config=config,
        )
    except Exception as e:
        log.error("pilot: gemini turn 1 failed", error=str(e))
        return {
            "filters": None,
            "message": "I'm having trouble connecting right now. Please try again in a moment.",
        }

    filters_applied: dict | None = None

    if response.function_calls:
        fn_call = response.function_calls[0]
        if fn_call.name == "apply_filters":
            # Extract and sanitize filter args
            filters_applied = {k: v for k, v in fn_call.args.items()}

            # Turn 2: Send function result back to get Sage's confirmation response
            try:
                contents.append(response.candidates[0].content)
                contents.append(types.Content(
                    role="user",
                    parts=[types.Part.from_function_response(
                        name="apply_filters",
                        response={"result": "Filters applied successfully."},
                    )]
                ))
                final = client.models.generate_content(
                    model=GENERATION_MODEL,
                    contents=contents,
                    config=config,
                )
                confirmation = final.text or "Done! I've updated your search filters."
            except Exception as e:
                log.error("pilot: gemini turn 2 failed", error=str(e))
                confirmation = "I've updated your filters! Check the results."
        else:
            # Unknown function — fall back to text
            confirmation = response.text or "I'm not sure how to help with that. Could you tell me more?"
            filters_applied = None
    else:
        # No function call — clarification, greeting, or unknown request
        confirmation = response.text or "I'm not sure what you're looking for. Could you tell me more about the type of expert you need?"

    log.info(
        "pilot: request processed",
        has_filters=filters_applied is not None,
        message_length=len(confirmation),
    )

    return {"filters": filters_applied, "message": confirmation}
```
  </action>
  <verify>
    grep -n "run_pilot\|APPLY_FILTERS_DECLARATION\|function_calls" app/services/pilot_service.py — all three appear.
    python -c "from app.services.pilot_service import run_pilot; print('OK')" — imports without error.
  </verify>
  <done>
    pilot_service.py created with two-turn Gemini function calling pattern.
    APPLY_FILTERS_DECLARATION declares query, rate_min, rate_max, tags, reset parameters.
    run_pilot makes Turn 1 to extract function call, Turn 2 to get confirmation text.
    Error handling returns graceful fallback messages.
    Imports without error.
  </done>
</task>

<task type="auto">
  <name>Task 2: Create pilot.py router and register in main.py</name>
  <files>
    app/routers/pilot.py
    app/main.py
  </files>
  <action>
Create app/routers/pilot.py following the exact pattern of explore.py (run_in_executor for sync service call):

```python
"""
POST /api/pilot — Sage co-pilot endpoint.

Receives user message + conversation history + current filter state.
Uses Gemini function calling (two-turn) to:
1. Extract apply_filters args from natural language
2. Generate Sage's confirmation response

Returns:
    {"filters": dict | null, "message": str}
    - filters: apply_filters args if filter intent detected, else null
    - message: Sage's natural language response to display in chat
"""
import asyncio
from typing import Literal

from fastapi import APIRouter
from pydantic import BaseModel, Field

from app.services.pilot_service import run_pilot

router = APIRouter()


class HistoryItem(BaseModel):
    role: Literal["user", "model"]  # Gemini roles — 'model' not 'assistant'
    content: str


class PilotRequest(BaseModel):
    message: str = Field(..., min_length=1, max_length=2000)
    history: list[HistoryItem] = Field(default_factory=list)
    current_filters: dict = Field(default_factory=dict)


class PilotResponse(BaseModel):
    filters: dict | None  # apply_filters args, or null if no filter change
    message: str          # Sage's natural language response


@router.post("/api/pilot", response_model=PilotResponse)
async def pilot(body: PilotRequest) -> PilotResponse:
    """
    Sage co-pilot endpoint — thin async wrapper around pilot_service.run_pilot().
    Offloads synchronous Gemini call to thread pool (same pattern as explore.py).
    """
    loop = asyncio.get_event_loop()
    result = await loop.run_in_executor(
        None,
        lambda: run_pilot(
            message=body.message,
            history=[h.model_dump() for h in body.history],
            current_filters=body.current_filters,
        ),
    )
    return PilotResponse(**result)
```

Then update app/main.py:
1. Add import: `from app.routers import ..., pilot` (add `pilot` to the existing router import line)
2. Add at the end of the routes section: `app.include_router(pilot.router)`

The existing CORS configuration already includes `Content-Type` in allow_headers — no CORS changes needed.
  </action>
  <verify>
    grep -n "api/pilot" app/routers/pilot.py — returns the route declaration.
    grep -n "pilot" app/main.py — returns both the import and include_router lines.
    curl -s -X POST http://localhost:8000/api/pilot -H "Content-Type: application/json" -d '{"message":"show marketing experts"}' | python -c "import sys,json; d=json.load(sys.stdin); print('OK' if 'message' in d else 'FAIL')"
    If backend not running: python -c "from app.routers.pilot import router; print('OK')" — imports without error.
  </verify>
  <done>
    pilot.py router created with POST /api/pilot accepting PilotRequest (message, history, current_filters).
    Response model PilotResponse has filters (dict | None) and message (str).
    main.py imports and registers pilot.router.
    Endpoint callable and returns valid JSON response.
  </done>
</task>

</tasks>

<verification>
Test the pilot endpoint:
curl -s -X POST http://localhost:8000/api/pilot -H "Content-Type: application/json" \
  -d '{"message":"show marketing experts under 100 per hour","history":[],"current_filters":{"query":"","rate_min":0,"rate_max":5000,"tags":[]}}' \
  | python -c "import sys,json; d=json.load(sys.stdin); print('filters:', d.get('filters')); print('message:', d.get('message')[:80])"

Expected: filters contains query or tags or rate_max fields, message is a friendly Sage response.

python -c "from app.routers.pilot import router; from app.services.pilot_service import run_pilot; print('imports OK')"
grep "pilot" app/main.py — shows both import and include_router.
</verification>

<success_criteria>
- POST /api/pilot is registered and callable
- Returns {"filters": {...} | null, "message": "string"} for any input
- When filter intent given: filters contains apply_filters args (query, tags, rate_min, rate_max, and/or reset)
- When no filter intent (greeting/clarification): filters is null, message is Sage's response
- Error handling: network failures return graceful fallback message, not 500
- Uses run_in_executor async wrapper (non-blocking, same pattern as explore.py)
</success_criteria>

<output>
After completion, create `.planning/phases/18-floating-ai-co-pilot/18-01-SUMMARY.md`
</output>
