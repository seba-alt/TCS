---
phase: 14-hybrid-search-backend
plan: "03"
type: execute
wave: 1
depends_on: []
files_modified:
  - app/services/explorer.py
autonomous: true
gap_closure: true
requirements:
  - EXPL-04

must_haves:
  truths:
    - "run_explore() applies feedback boost multipliers to the scored list after findability boost, before sort"
    - "Experts with fewer than 10 total feedback interactions (up + down) receive no boost (cold-start guard)"
    - "A DB error during feedback loading logs a warning and returns the scored list unchanged — never raises"
    - "Experts with no profile_url are silently skipped during feedback lookup"
  artifacts:
    - path: "app/services/explorer.py"
      provides: "Inline feedback boost in run_explore() text-query branch"
      contains: "Feedback"
      exports: ["run_explore", "ExploreResponse", "ExpertCard"]
  key_links:
    - from: "app/services/explorer.py"
      to: "app.models.Feedback"
      via: "from app.models import Expert, Feedback"
      pattern: "from app\\.models import.*Feedback"
    - from: "app/services/explorer.py"
      to: "feedback table"
      via: "db.scalars(select(Feedback).where(...))"
      pattern: "select\\(Feedback\\)"
---

<objective>
Close the single gap blocking EXPL-04: add inline feedback boost logic to `run_explore()` in `app/services/explorer.py`.

Purpose: ROADMAP success criterion 3 and EXPL-04 both require findability AND feedback boosts applied on top of fused rankings. Findability boost is present; feedback boost is absent.

Output: `run_explore()` applies a feedback multiplier to each entry in `scored` after the findability boost loop completes and before `scored.sort(...)`. Logic is inline (no import of `_apply_feedback_boost` — type mismatch), matches the formula and guards from `search_intelligence._apply_feedback_boost()`.
</objective>

<execution_context>
@./.claude/get-shit-done/workflows/execute-plan.md
@./.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/14-hybrid-search-backend/14-CONTEXT.md
@.planning/phases/14-hybrid-search-backend/14-VERIFICATION.md
</context>

<tasks>

<task type="auto">
  <name>Task 1: Add inline feedback boost to run_explore()</name>
  <files>app/services/explorer.py</files>
  <action>
Make two changes to `app/services/explorer.py`:

**Change 1 — Import `Feedback` model.**

Update the existing import at line 27 from:
```python
from app.models import Expert
```
to:
```python
from app.models import Expert, Feedback
```

**Change 2 — Insert inline feedback boost block.**

After the score-fusion loop (which ends at line 249 with `scored.append(...)`) and BEFORE `scored.sort(key=lambda x: x[0], reverse=True)` (currently line 251), insert the following block.

The insertion point is the gap between `scored.append((final, fs, bs, expert))` and `scored.sort(...)`. Insert exactly here:

```python
        # --- Feedback boost (inline — type-compatible with scored list) ---
        # Mirrors the formula in search_intelligence._apply_feedback_boost().
        # Graceful degradation: any DB error logs a warning and returns scored unchanged.
        try:
            url_set = {
                e.profile_url_utm or e.profile_url
                for _, _, _, e in scored
                if e.profile_url_utm or e.profile_url
            }
            if url_set:
                feedback_rows = db.scalars(
                    select(Feedback).where(Feedback.vote.in_(["up", "down"]))
                ).all()

                counts: dict[str, dict[str, int]] = {u: {"up": 0, "down": 0} for u in url_set}
                for row in feedback_rows:
                    expert_ids = json.loads(row.expert_ids or "[]")
                    for eid in expert_ids:
                        if eid in url_set:
                            counts[eid][row.vote] = counts[eid].get(row.vote, 0) + 1

                FEEDBACK_BOOST_CAP = 0.20
                boost_factor = FEEDBACK_BOOST_CAP * 2  # 0.40 — mirrors search_intelligence formula

                multipliers: dict[str, float] = {}
                for url in url_set:
                    up = counts[url]["up"]
                    down = counts[url]["down"]
                    total_votes = up + down
                    if total_votes < 10:
                        continue  # cold-start guard — skip sparse feedback
                    ratio = up / total_votes
                    if ratio > 0.5:
                        multipliers[url] = 1.0 + (ratio - 0.5) * boost_factor
                    elif ratio < 0.5:
                        multipliers[url] = 1.0 - (0.5 - ratio) * boost_factor

                if multipliers:
                    scored = [
                        (
                            final_s * multipliers.get(
                                e.profile_url_utm or e.profile_url, 1.0
                            ),
                            faiss_s,
                            bm25_s,
                            e,
                        )
                        for final_s, faiss_s, bm25_s, e in scored
                    ]
        except Exception as exc:
            log.warning("explore.feedback_boost_failed", error=str(exc))
            # scored unchanged — degrade gracefully, never raise
```

Important: `json` is already imported at line 15. `select` is already imported at line 24. `Feedback` will be imported by Change 1. No other imports needed.

The `scored.sort(key=lambda x: x[0], reverse=True)` line that follows must remain in place — it sorts AFTER the boost is applied.

Do NOT modify anything in the pure filter branch (the `else:` block starting at line 264), the `_build_card` helper, or any other functions.
  </action>
  <verify>
Run from the project root:

```bash
python -c "
import ast, sys
with open('app/services/explorer.py') as f:
    src = f.read()

# 1. Feedback import present
assert 'from app.models import Expert, Feedback' in src, 'FAIL: Feedback not imported'

# 2. Feedback boost block present
assert 'select(Feedback)' in src, 'FAIL: Feedback query missing'
assert 'cold-start guard' in src, 'FAIL: cold-start guard comment missing'
assert 'explore.feedback_boost_failed' in src, 'FAIL: graceful degradation log missing'
assert 'total_votes < 10' in src, 'FAIL: cold-start threshold missing'
assert 'FEEDBACK_BOOST_CAP' in src, 'FAIL: boost cap constant missing'

# 3. File is valid Python
ast.parse(src)

print('OK: all checks passed')
"
```

Also run the FastAPI app import check:
```bash
python -c "from app.services.explorer import run_explore, ExploreResponse, ExpertCard; print('OK: imports work')"
```
  </verify>
  <done>
Both commands print OK with no errors. The feedback boost block is present between the findability boost loop and `scored.sort(...)`. The Feedback model is imported. The file is syntactically valid Python.
  </done>
</task>

</tasks>

<verification>
After the task completes:

1. `python -c "from app.services.explorer import run_explore, ExploreResponse, ExpertCard"` exits 0.
2. `python -c "import ast; ast.parse(open('app/services/explorer.py').read())"` exits 0.
3. `grep -n "select(Feedback)" app/services/explorer.py` shows exactly one match inside `run_explore()`.
4. `grep -n "total_votes < 10" app/services/explorer.py` shows the cold-start guard.
5. `grep -n "explore.feedback_boost_failed" app/services/explorer.py` shows the degradation log.
6. `grep -n "scored.sort" app/services/explorer.py` shows the sort AFTER the feedback block.

EXPL-04 is now fully satisfied: both findability boost and feedback boost are applied on top of fused rankings.
</verification>

<success_criteria>
- `run_explore()` in `app/services/explorer.py` queries the Feedback table and applies multipliers to `scored` entries in the text-query branch
- Cold-start guard: experts with `up + down < 10` receive multiplier 1.0 (no boost, no penalty)
- Boost formula: `ratio = up / (up + down)`, `boost_factor = 0.40`, multiplier = `1 + (ratio-0.5)*0.40` if ratio > 0.5, `1 - (0.5-ratio)*0.40` if ratio < 0.5
- Any exception during feedback loading logs `explore.feedback_boost_failed` and returns `scored` unchanged
- File is valid Python with no import errors
- EXPL-04 fully satisfied — VERIFICATION.md truth 3 will pass on re-verification
</success_criteria>

<output>
After completion, create `.planning/phases/14-hybrid-search-backend/14-03-SUMMARY.md` following the summary template.
</output>
