---
phase: 31-admin-marketplace-intelligence
plan: "01"
type: execute
wave: 1
depends_on: []
files_modified:
  - app/routers/admin.py
autonomous: true
requirements:
  - INTEL-01
  - INTEL-02
  - INTEL-03
  - INTEL-04

must_haves:
  truths:
    - "GET /api/admin/events/demand returns zero-result Sage query rows grouped by query_text, sorted by frequency DESC"
    - "GET /api/admin/events/exposure returns expert click counts broken down by grid vs sage_panel context"
    - "GET /api/admin/events/trend returns daily Sage query totals split into hits vs zero_results, plus KPI fields for 14-day window"
    - "All three endpoints return data_since field (ISO timestamp or null) for cold-start detection"
    - "GET /api/admin/export/demand.csv and /export/exposure.csv return StreamingResponse with correct headers"
    - "All endpoints are protected by _require_admin dependency via the existing router object"
  artifacts:
    - path: "app/routers/admin.py"
      provides: "demand, exposure, trend, and CSV export endpoints"
      contains: "router.get(\"/events/demand\")"
  key_links:
    - from: "app/routers/admin.py"
      to: "user_events table"
      via: "sqlalchemy.text() with json_extract"
      pattern: "json_extract\\(payload"
    - from: "demand endpoint"
      to: "data_since field"
      via: "SELECT MIN(created_at) FROM user_events"
      pattern: "MIN\\(created_at\\)"
---

<objective>
Add three read-only aggregation endpoints and two CSV export endpoints to admin.py for the Marketplace Intelligence page. These endpoints read from the user_events table created in Phase 30.

Purpose: Give the frontend all data it needs to render the demand table, exposure table, daily trend chart, and cold-start empty state.
Output: Five new GET endpoints in app/routers/admin.py, all protected by _require_admin dependency.
</objective>

<execution_context>
@/Users/sebastianhamers/.claude/get-shit-done/workflows/execute-plan.md
@/Users/sebastianhamers/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/31-admin-marketplace-intelligence/31-CONTEXT.md
@.planning/phases/31-admin-marketplace-intelligence/31-RESEARCH.md
@app/routers/admin.py
@app/models.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Add demand and exposure aggregation endpoints</name>
  <files>app/routers/admin.py</files>
  <action>
Add three new endpoints to admin.py using the existing `router` object (line ~199: `router = APIRouter(prefix="/api/admin", dependencies=[Depends(_require_admin)])`). Do NOT add a new router — all endpoints go on the existing `router`.

Add required imports at the top of the file alongside existing imports:
```python
import io
import csv
from datetime import datetime, timedelta
from fastapi.responses import StreamingResponse
```
(Check if any of these are already imported and skip if so.)

**Endpoint 1: GET /events/demand**
```python
@router.get("/events/demand")
def get_demand(days: int = 30, page: int = 0, page_size: int = 25, db: Session = Depends(get_db)):
    from sqlalchemy import text as _text
    # Cold-start check
    earliest = db.execute(_text("SELECT MIN(created_at) FROM user_events")).scalar()
    data_since = earliest  # None if table empty, ISO string otherwise

    cutoff = (datetime.utcnow() - timedelta(days=days)).strftime("%Y-%m-%d") if days > 0 else "2000-01-01"

    total_row = db.execute(_text("""
        SELECT COUNT(DISTINCT json_extract(payload, '$.query_text')) AS cnt
        FROM user_events
        WHERE event_type = 'sage_query'
          AND json_extract(payload, '$.zero_results') = 1
          AND date(created_at) >= :cutoff
    """), {"cutoff": cutoff}).scalar()
    total = total_row or 0

    rows = db.execute(_text("""
        SELECT
            json_extract(payload, '$.query_text') AS query_text,
            COUNT(*) AS frequency,
            MAX(created_at) AS last_seen,
            COUNT(DISTINCT session_id) AS unique_users
        FROM user_events
        WHERE event_type = 'sage_query'
          AND json_extract(payload, '$.zero_results') = 1
          AND date(created_at) >= :cutoff
        GROUP BY json_extract(payload, '$.query_text')
        ORDER BY frequency DESC
        LIMIT :limit OFFSET :offset
    """), {"cutoff": cutoff, "limit": page_size, "offset": page * page_size}).all()

    return {
        "data_since": data_since,
        "demand": [{"query_text": r.query_text, "frequency": r.frequency, "last_seen": r.last_seen, "unique_users": r.unique_users} for r in rows],
        "total": total,
        "page": page,
        "page_size": page_size,
    }
```

**Endpoint 2: GET /events/exposure**
```python
@router.get("/events/exposure")
def get_exposure(days: int = 30, db: Session = Depends(get_db)):
    from sqlalchemy import text as _text
    earliest = db.execute(_text("SELECT MIN(created_at) FROM user_events")).scalar()
    data_since = earliest

    cutoff = (datetime.utcnow() - timedelta(days=days)).strftime("%Y-%m-%d") if days > 0 else "2000-01-01"

    rows = db.execute(_text("""
        SELECT
            json_extract(payload, '$.expert_id') AS expert_id,
            COUNT(*) AS total_clicks,
            SUM(CASE WHEN json_extract(payload, '$.context') = 'grid' THEN 1 ELSE 0 END) AS grid_clicks,
            SUM(CASE WHEN json_extract(payload, '$.context') = 'sage_panel' THEN 1 ELSE 0 END) AS sage_clicks
        FROM user_events
        WHERE event_type = 'card_click'
          AND date(created_at) >= :cutoff
        GROUP BY json_extract(payload, '$.expert_id')
        HAVING total_clicks > 0
        ORDER BY total_clicks DESC
    """), {"cutoff": cutoff}).all()

    return {
        "data_since": data_since,
        "exposure": [{"expert_id": r.expert_id, "total_clicks": r.total_clicks, "grid_clicks": r.grid_clicks, "sage_clicks": r.sage_clicks} for r in rows],
    }
```

**Endpoint 3: GET /events/trend**

This returns daily Sage query data for the stacked BarChart plus headline KPIs. Fixed 14-day window for daily data; KPIs use 14-day current + 14-day prior period for change calculation.

```python
@router.get("/events/trend")
def get_trend(db: Session = Depends(get_db)):
    from sqlalchemy import text as _text
    earliest = db.execute(_text("SELECT MIN(created_at) FROM user_events")).scalar()
    data_since = earliest

    cutoff_14d = (datetime.utcnow() - timedelta(days=14)).strftime("%Y-%m-%d")
    cutoff_28d = (datetime.utcnow() - timedelta(days=28)).strftime("%Y-%m-%d")

    daily_rows = db.execute(_text("""
        SELECT
            strftime('%Y-%m-%d', created_at) AS day,
            COUNT(*) AS total,
            SUM(CASE WHEN json_extract(payload, '$.zero_results') = 1 THEN 1 ELSE 0 END) AS zero_results,
            SUM(CASE WHEN json_extract(payload, '$.zero_results') = 0 THEN 1 ELSE 0 END) AS hits
        FROM user_events
        WHERE event_type = 'sage_query'
          AND date(created_at) >= :cutoff
        GROUP BY strftime('%Y-%m-%d', created_at)
        ORDER BY day
    """), {"cutoff": cutoff_14d}).all()

    current_total = db.execute(_text("""
        SELECT COUNT(*) FROM user_events
        WHERE event_type = 'sage_query' AND date(created_at) >= :cutoff
    """), {"cutoff": cutoff_14d}).scalar() or 0

    current_zero = db.execute(_text("""
        SELECT COUNT(*) FROM user_events
        WHERE event_type = 'sage_query'
          AND json_extract(payload, '$.zero_results') = 1
          AND date(created_at) >= :cutoff
    """), {"cutoff": cutoff_14d}).scalar() or 0

    prior_total = db.execute(_text("""
        SELECT COUNT(*) FROM user_events
        WHERE event_type = 'sage_query'
          AND date(created_at) >= :prior AND date(created_at) < :cutoff
    """), {"prior": cutoff_28d, "cutoff": cutoff_14d}).scalar() or 0

    zero_result_rate = round(current_zero / current_total * 100, 1) if current_total > 0 else 0.0

    return {
        "data_since": data_since,
        "daily": [{"day": r.day, "total": r.total, "hits": r.hits, "zero_results": r.zero_results} for r in daily_rows],
        "kpis": {
            "total_queries": current_total,
            "zero_result_rate": zero_result_rate,
            "prior_period_total": prior_total,
        },
    }
```

Place these three endpoints in a clearly commented section: `# --- Marketplace Intelligence endpoints ---`, near the bottom of admin.py before any CSV export section.

CRITICAL: Use `json_extract(payload, '$.zero_results') = 1` (not `= true` or `= True`) — SQLite stores JSON booleans as integers.
  </action>
  <verify>
Run from /Users/sebastianhamers/Documents/TCS:
```bash
cd /Users/sebastianhamers/Documents/TCS && python -c "import app.routers.admin; print('import ok')"
```
If local backend is running, also test:
```bash
curl -s -H "X-Admin-Key: test" http://localhost:8000/api/admin/events/demand | python -m json.tool | head -20
curl -s -H "X-Admin-Key: test" http://localhost:8000/api/admin/events/exposure | python -m json.tool | head -10
curl -s -H "X-Admin-Key: test" http://localhost:8000/api/admin/events/trend | python -m json.tool | head -20
```
  </verify>
  <done>
Three endpoints exist in admin.py under the existing router object. Module imports cleanly. Each endpoint returns data_since, plus the appropriate data array. json_extract uses = 1 for boolean comparison.
  </done>
</task>

<task type="auto">
  <name>Task 2: Add CSV export endpoints for demand and exposure tables</name>
  <files>app/routers/admin.py</files>
  <action>
Add two CSV export endpoints following the existing StreamingResponse pattern used by `/export/searches.csv` and `/export/gaps.csv` in admin.py.

Verify `import io` and `import csv` are at the top (added in Task 1 or already present). Also verify `from datetime import date` is imported (needed for filename generation).

**Endpoint 4: GET /export/demand.csv**
```python
@router.get("/export/demand.csv")
def export_demand_csv(days: int = 30, db: Session = Depends(get_db)):
    from sqlalchemy import text as _text
    from datetime import date as _date
    cutoff = (datetime.utcnow() - timedelta(days=days)).strftime("%Y-%m-%d") if days > 0 else "2000-01-01"
    rows = db.execute(_text("""
        SELECT
            json_extract(payload, '$.query_text') AS query_text,
            COUNT(*) AS frequency,
            MAX(created_at) AS last_seen,
            COUNT(DISTINCT session_id) AS unique_users
        FROM user_events
        WHERE event_type = 'sage_query'
          AND json_extract(payload, '$.zero_results') = 1
          AND date(created_at) >= :cutoff
        GROUP BY json_extract(payload, '$.query_text')
        ORDER BY frequency DESC
    """), {"cutoff": cutoff}).all()

    buf = io.StringIO()
    writer = csv.writer(buf)
    writer.writerow(["# Export date", _date.today().isoformat()])
    writer.writerow(["# Days window", days])
    writer.writerow([])
    writer.writerow(["query_text", "frequency", "last_seen", "unique_users"])
    for r in rows:
        writer.writerow([r.query_text or "", r.frequency, r.last_seen or "", r.unique_users])

    filename = f"demand-{_date.today().isoformat()}.csv"
    return StreamingResponse(
        iter([buf.getvalue()]),
        media_type="text/csv",
        headers={"Content-Disposition": f"attachment; filename={filename}"},
    )
```

**Endpoint 5: GET /export/exposure.csv**
```python
@router.get("/export/exposure.csv")
def export_exposure_csv(days: int = 30, db: Session = Depends(get_db)):
    from sqlalchemy import text as _text
    from datetime import date as _date
    cutoff = (datetime.utcnow() - timedelta(days=days)).strftime("%Y-%m-%d") if days > 0 else "2000-01-01"
    rows = db.execute(_text("""
        SELECT
            json_extract(payload, '$.expert_id') AS expert_id,
            COUNT(*) AS total_clicks,
            SUM(CASE WHEN json_extract(payload, '$.context') = 'grid' THEN 1 ELSE 0 END) AS grid_clicks,
            SUM(CASE WHEN json_extract(payload, '$.context') = 'sage_panel' THEN 1 ELSE 0 END) AS sage_clicks
        FROM user_events
        WHERE event_type = 'card_click'
          AND date(created_at) >= :cutoff
        GROUP BY json_extract(payload, '$.expert_id')
        HAVING total_clicks > 0
        ORDER BY total_clicks DESC
    """), {"cutoff": cutoff}).all()

    buf = io.StringIO()
    writer = csv.writer(buf)
    writer.writerow(["# Export date", _date.today().isoformat()])
    writer.writerow(["# Days window", days])
    writer.writerow([])
    writer.writerow(["expert_id", "total_clicks", "grid_clicks", "sage_clicks"])
    for r in rows:
        writer.writerow([r.expert_id or "", r.total_clicks, r.grid_clicks, r.sage_clicks])

    filename = f"exposure-{_date.today().isoformat()}.csv"
    return StreamingResponse(
        iter([buf.getvalue()]),
        media_type="text/csv",
        headers={"Content-Disposition": f"attachment; filename={filename}"},
    )
```

Place both endpoints in the same `# --- Marketplace Intelligence endpoints ---` section immediately after the three aggregation endpoints from Task 1.
  </action>
  <verify>
```bash
cd /Users/sebastianhamers/Documents/TCS && python -c "import app.routers.admin; print('import ok')"
```
If backend running:
```bash
curl -s -H "X-Admin-Key: test" "http://localhost:8000/api/admin/export/demand.csv" | head -8
curl -s -H "X-Admin-Key: test" "http://localhost:8000/api/admin/export/exposure.csv" | head -8
```
Confirm: Content-Disposition header present, rows have correct column names.
  </verify>
  <done>
Two CSV export endpoints exist in admin.py. Both follow the existing StreamingResponse + metadata-header-rows pattern. Module imports cleanly with no errors.
  </done>
</task>

</tasks>

<verification>
1. `python -c "import app.routers.admin; print('import ok')"` runs without errors
2. Five new endpoints exist under the `# --- Marketplace Intelligence endpoints ---` comment section in admin.py
3. All five endpoints use `router.get(...)` — NOT a new router object
4. Boolean json_extract comparison uses `= 1` not `= true`
5. `data_since` field present in demand, exposure, and trend responses
6. CSV exports use metadata header rows (# Export date, # Days window) consistent with existing exports
</verification>

<success_criteria>
- Five endpoints registered on existing `router` object: GET /events/demand, GET /events/exposure, GET /events/trend, GET /export/demand.csv, GET /export/exposure.csv
- All protected by _require_admin (inherited from router-level dependency)
- demand endpoint: returns data_since, demand[], total, page, page_size with ?days=30&page=0 defaults
- exposure endpoint: returns data_since, exposure[] with expert_id/total_clicks/grid_clicks/sage_clicks
- trend endpoint: returns data_since, daily[] (14d), kpis{total_queries, zero_result_rate, prior_period_total}
- Admin.py imports cleanly
</success_criteria>

<output>
After completion, create `.planning/phases/31-admin-marketplace-intelligence/31-01-SUMMARY.md`
</output>
