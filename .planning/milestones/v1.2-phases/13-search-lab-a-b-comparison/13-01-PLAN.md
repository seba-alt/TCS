---
phase: 13-search-lab-a-b-comparison
plan: "01"
type: execute
wave: 1
depends_on: []
files_modified:
  - app/routers/admin.py
  - frontend/src/admin/types.ts
autonomous: true
requirements: [LAB-01, LAB-04]

must_haves:
  truths:
    - "POST /api/admin/compare accepts a query, up to 4 config flags, and per-run overrides; returns ranked results for each config without writing to the DB"
    - "Per-run overrides (hyde_override, feedback_override) are applied only to the retrieval call; GET /api/admin/settings returns unchanged values after the call"
    - "TypeScript CompareColumn, CompareExpert, CompareResponse, LabConfig, and OverrideFlags types are exported from types.ts and reflect the backend response shape"
  artifacts:
    - path: "app/routers/admin.py"
      provides: "POST /api/admin/compare endpoint"
      contains: "compare"
    - path: "frontend/src/admin/types.ts"
      provides: "A/B comparison TypeScript types"
      contains: "CompareResponse"
  key_links:
    - from: "app/routers/admin.py"
      to: "app/services/search_intelligence.retrieve_with_intelligence"
      via: "ThreadPoolExecutor parallel invocation per config"
      pattern: "retrieve_with_intelligence"
    - from: "frontend/src/admin/types.ts"
      to: "POST /api/admin/compare"
      via: "CompareResponse interface matches backend JSON shape"
      pattern: "CompareResponse"
---

<objective>
Add a POST /api/admin/compare endpoint that runs the same query through up to 4 preset intelligence configurations in parallel and returns ranked results per config — without touching global DB settings. Also add the TypeScript types that the frontend will consume in Plan 02.

Purpose: Provides the data contract and backend logic for A/B comparison. Per-run overrides must be implemented here so the frontend can force-enable features for a single run without calling POST /api/admin/settings.
Output: Working /api/admin/compare endpoint + CompareResponse TypeScript types.
</objective>

<execution_context>
@/Users/sebastianhamers/.claude/get-shit-done/workflows/execute-plan.md
@/Users/sebastianhamers/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md

@app/routers/admin.py
@app/services/search_intelligence.py
@app/services/retriever.py
@app/routers/chat.py
@frontend/src/admin/types.ts
@.planning/phases/13-search-lab-a-b-comparison/13-CONTEXT.md
</context>

<tasks>

<task type="auto">
  <name>Task 1: Add POST /api/admin/compare to admin.py</name>
  <files>app/routers/admin.py</files>
  <action>
Add a new section "# ── Search Lab A/B Compare ──" near the bottom of app/routers/admin.py (after Settings, before end of file).

**Request model (Pydantic):**
```python
class CompareRequest(BaseModel):
    query: str = Field(..., min_length=1, max_length=2000)
    configs: list[str] = Field(
        default=["baseline", "hyde", "feedback", "full"],
        description="Which preset configs to run. Valid values: baseline, hyde, feedback, full."
    )
    result_count: int = Field(default=20, ge=1, le=50)
    overrides: dict[str, bool] = Field(
        default_factory=dict,
        description="Per-run flag overrides applied on top of DB settings. Keys: QUERY_EXPANSION_ENABLED, FEEDBACK_LEARNING_ENABLED."
    )
```

**Config presets (module-level dict, defined before the endpoint):**
```python
_LAB_CONFIGS = {
    "baseline": {"QUERY_EXPANSION_ENABLED": False, "FEEDBACK_LEARNING_ENABLED": False},
    "hyde":     {"QUERY_EXPANSION_ENABLED": True,  "FEEDBACK_LEARNING_ENABLED": False},
    "feedback": {"QUERY_EXPANSION_ENABLED": False, "FEEDBACK_LEARNING_ENABLED": True},
    "full":     {"QUERY_EXPANSION_ENABLED": True,  "FEEDBACK_LEARNING_ENABLED": True},
}
```

**Retrieval logic for a single config:**

The existing `retrieve_with_intelligence()` reads settings from the DB on every call — we need to bypass that for lab runs. Add a private helper `_retrieve_for_lab(query, faiss_index, metadata, db, config_flags, result_count)` that:
1. Calls `get_settings(db)` to get current DB settings.
2. Merges `config_flags` on top of those settings: `settings.update(config_flags)` — this applies the preset + any per-run `overrides` baked into `config_flags`. The DB is NOT written.
3. Calls the low-level retrieval logic inline (do NOT call `retrieve_with_intelligence()` which reads DB again). Instead replicate the retrieval steps from `retrieve_with_intelligence()` but use the merged `settings` dict directly:
   - Step 1: `candidates = retrieve(query, faiss_index, metadata)` (from `app.services.retriever`)
   - Step 2: If `settings["QUERY_EXPANSION_ENABLED"]` and `_is_weak_query(candidates, settings["STRONG_RESULT_MIN"], settings["SIMILARITY_THRESHOLD"])`: run HyDE expansion — import `_generate_hypothetical_bio`, `_blend_embeddings`, `_search_with_vector`, `_merge_candidates` from `app.services.search_intelligence`.
   - Step 3: If `settings["FEEDBACK_LEARNING_ENABLED"]`: run `_apply_feedback_boost(candidates, db, settings["FEEDBACK_BOOST_CAP"])`.
4. Return `(candidates[:result_count], intelligence_meta)`.

Import at top of the new section (deferred inside functions is fine for the internals, but `retrieve` and helpers from search_intelligence can be imported at module top):
```python
from app.services.retriever import retrieve
from app.services.search_intelligence import (
    get_settings, _is_weak_query, _generate_hypothetical_bio,
    _blend_embeddings, _search_with_vector, _merge_candidates, _apply_feedback_boost,
)
```

**Endpoint:**
```python
@router.post("/compare")
def compare_configs(
    body: CompareRequest,
    request: Request,
    db: Session = Depends(get_db),
):
```

- Validate `body.configs`: each entry must be in `_LAB_CONFIGS`. Return HTTP 400 for unknown config names.
- Build per-config flag dicts: `config_flags = {**_LAB_CONFIGS[name], **body.overrides}` for each config name. The `overrides` from the request are applied uniformly on top of each preset (per CONTEXT.md: override scope applies to all selected configs).
- Run retrieval for all configs using `concurrent.futures.ThreadPoolExecutor` to execute in parallel. Each thread calls `_retrieve_for_lab(...)`. Import `from concurrent.futures import ThreadPoolExecutor` at top of file (or inside function).
- Collect results. For each config, serialize candidates as:
  ```python
  {
      "config": config_name,
      "label": {"baseline": "Baseline", "hyde": "HyDE Only", "feedback": "Feedback Only", "full": "Full Intelligence"}[config_name],
      "experts": [
          {
              "rank": i + 1,
              "name": c.name,
              "title": c.title,
              "score": round(c.score, 4),
              "profile_url": c.profile_url,
          }
          for i, c in enumerate(candidates)
      ],
      "intelligence": intelligence_meta,
  }
  ```
- Return JSON: `{"columns": [...], "query": body.query, "overrides_applied": body.overrides}`.
- Do NOT write to DB (no Conversation row, no settings writes).

**ruff compliance:** Run `ruff check app/routers/admin.py --fix` after writing to ensure no lint errors. The private imports from `search_intelligence` (prefixed `_`) may trigger linting — add `# noqa: PLC2701` comment if needed.
  </action>
  <verify>
    Run: `cd /Users/sebastianhamers/Documents/TCS && python -c "from app.routers.admin import router; print('admin.py OK')" 2>&1`

    Run: `cd /Users/sebastianhamers/Documents/TCS && ruff check app/routers/admin.py 2>&1`

    If backend dev server is running: `curl -s -X POST http://localhost:8000/api/admin/compare -H "Content-Type: application/json" -H "X-Admin-Key: $(cat .env | grep ADMIN_KEY | cut -d= -f2)" -d '{"query":"help me raise a Series A","configs":["baseline","full"],"result_count":5}' | python3 -m json.tool | head -40 2>&1`
  </verify>
  <done>
    admin.py imports cleanly (no Python errors), ruff passes with no errors, and a curl to /api/admin/compare with valid query + configs returns JSON with a "columns" array containing one entry per config, each with "experts" ranked list and "intelligence" metadata. GET /api/admin/settings after the compare call returns the same values as before (no DB mutation).
  </done>
</task>

<task type="auto">
  <name>Task 2: Add A/B comparison TypeScript types to types.ts</name>
  <files>frontend/src/admin/types.ts</files>
  <action>
Append to the bottom of `frontend/src/admin/types.ts` (after AdminSettingsResponse):

```typescript
// ── Search Lab A/B Comparison ─────────────────────────────────────────────────

/** One expert result row in a comparison column. */
export interface CompareExpert {
  rank: number
  name: string
  title: string | null
  score: number
  profile_url: string | null
}

/** One config column result from POST /api/admin/compare. */
export interface CompareColumn {
  config: 'baseline' | 'hyde' | 'feedback' | 'full'
  label: string   // e.g. "Baseline", "HyDE Only", "Feedback Only", "Full Intelligence"
  experts: CompareExpert[]
  intelligence: {
    hyde_triggered: boolean
    hyde_bio: string | null
    feedback_applied: boolean
  }
}

/** Full response from POST /api/admin/compare. */
export interface CompareResponse {
  columns: CompareColumn[]
  query: string
  overrides_applied: Record<string, boolean>
}

/** Which preset configs are selected for a lab run. */
export type LabConfigKey = 'baseline' | 'hyde' | 'feedback' | 'full'

/** Per-run flag overrides (applied on top of each preset, do not change global DB settings). */
export interface LabOverrides {
  QUERY_EXPANSION_ENABLED?: boolean
  FEEDBACK_LEARNING_ENABLED?: boolean
}
```

Run `cd /Users/sebastianhamers/Documents/TCS/frontend && npm run build 2>&1 | tail -20` to confirm TypeScript compiles.
  </action>
  <verify>
    Run: `cd /Users/sebastianhamers/Documents/TCS/frontend && npm run build 2>&1 | tail -15`
    Expect: Build succeeds with 0 TypeScript errors.
  </verify>
  <done>
    frontend/src/admin/types.ts exports CompareExpert, CompareColumn, CompareResponse, LabConfigKey, and LabOverrides. TypeScript build passes with no errors referencing the new types.
  </done>
</task>

</tasks>

<verification>
1. `python -c "from app.routers.admin import router"` runs without error.
2. `ruff check app/routers/admin.py` returns no errors.
3. `cd frontend && npm run build` succeeds with no TypeScript errors.
4. POST /api/admin/compare with `{"query":"...", "configs":["baseline","full"]}` returns `{"columns":[...], "query":"...", "overrides_applied":{}}`.
5. GET /api/admin/settings before and after the compare call returns identical JSON (DB not modified).
</verification>

<success_criteria>
- POST /api/admin/compare endpoint exists, requires X-Admin-Key, accepts query+configs+result_count+overrides, returns columns JSON.
- Per-run overrides are applied to config flags without writing to the settings table.
- TypeScript types (CompareResponse, CompareColumn, CompareExpert, LabConfigKey, LabOverrides) exported from types.ts.
- Both backend and frontend build cleanly.
</success_criteria>

<output>
After completion, create `.planning/phases/13-search-lab-a-b-comparison/13-01-SUMMARY.md`
</output>
